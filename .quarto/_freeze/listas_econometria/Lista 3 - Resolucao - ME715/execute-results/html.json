{
  "hash": "68e92b4083d6024953bd4a2d82391214",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: 'Lista 3 - Econometria - ME715'\nlang: pt\ncrossref:\n  eq-prefix: 'Eq. '\nformat:\n  html:\n    embed-resources: true\n    theme:\n      dark: darkly\n      light: flatly\n---\n\n\n::: {.hidden}\n\n::: {.cell}\n<style type=\"text/css\">\np {\n  text-align: justify\n}\n\n/* Ocultar o texto padrão de referência */\n.reference-cross {\n    display: none;\n}\n\n/* Estilo para mostrar apenas o número da equação */\n.equation-number {\n    color: #007BFF; /* Cor personalizada */\n    font-weight: bold; /* Negrito */\n}\n\ndetails {\n    border: 2px solid #272726; /* Borda ao redor do elemento */\n    border-radius: 5px; /* Arredondamento das bordas */\n    padding: 10px; /* Espaçamento interno */\n    margin-top: 10px; /* Espaçamento superior */\n}\n</style>\n:::\n\n\n$$\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\posto}{posto}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\Var}{\\mathbb{V}}\n\\DeclareMathOperator{\\bbE}{\\mathbb{E}}\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\newcommand{\\RR}{\\mathbb{R}}\n$$\n:::\n\n::: {.callout-note icon=false}\n# Questão 1\n\nProve que $\\boldsymbol M = I - X(X'X)^{-1}X'$ é simétrica e idempotente.\n:::\n\nVamos provar que $\\bs{M}$ é simétrica, isto é, $\\bs{M}' = \\bs{M}$:\n\n\n$$\n\\bs{M}' = (I - X(X'X)^{-1}X')' = I - X(X'X)^{-1}X' = \\bs{M}\n$$\n\nVamos provar que $\\bs{M}$ é idempotente, isto é, $\\bs{M}\\bs{M} = \\bs{M}$:\n\n\\begin{align*}\n  \\bs{M}\\bs{M} &= (I - X(X'X)^{-1}X') (I - X(X'X)^{-1}X')\\\\\n  &= I - X(X'X)^{-1}X' -  X(X'X)^{-1}X' + X(X'X)^{-1}X' X(X'X)^{-1}X'\\\\\n  &= I - 2 X(X'X)^{-1}X' + X(X'X)^{-1}X'\\\\\n  &= I - X(X'X)^{-1}X'\\\\\n  &= \\bs{M}\n\\end{align*}\n\nPortanto, fica provado que $\\bs{M}$ é simétrica e idempotente.\n\n<br><br>\n\n\n::: {.callout-note icon=false}\n# Questão 2\n\nMostre que $s^2 = \\displaystyle\\frac{\\hat u' \\hat u}{n-k-1}$ é um estimador não viesado para $\\sigma^2$.\n:::\n\n::: {.callout-tip}\n# Resultados auxiliares\n\n::: {#thm-esp_forma_quadratica}\n\n# esperança de forma quadrática\n\nSeja $X$ um vetor aleatório $n\\times 1$ com média $\\mu$ e covariância $\\Sigma$ e $A \\in \\mathbb{R}^{n \\times n}$ uma matriz simétrica. Então, a esperança de uma forma quadrática $X'AX$ é dada por:\n$$\\bbE(X'AX) = \\tr(A\\Sigma) + \\mu'A\\mu$$\n:::\n\n<details>\n<summary> Proof </summary>\nNote que $X'AX$ é uma matriz $1 \\times 1$, então\n$$\n\\mathbb E(X'AX) = \\bbE(\\tr(X'AX)) = \\bbE(\\tr(AXX')) = \\tr(A\\bbE(XX'))\n$$\n\nAlém disso, como $\\Var(X) = \\bbE(XX') - \\bbE(X)\\bbE(X)' \\Rightarrow \\bbE(XX') = \\Var(X) + \\bbE(X)\\bbE(X)'$, então\n\n\\begin{align}\n\\bbE(X'AX) &= \\tr(A\\bbE(XX'))\\\\\n           &= \\tr(A[\\Var(X) + \\bbE(X)\\bbE(X)'])\\\\\n           &= \\tr(A(\\Sigma + \\mu\\mu'))\\\\\n           &= \\tr(A\\Sigma) + \\tr(A\\mu\\mu')\\\\\n           &= \\tr(A\\Sigma) + \\tr(\\mu'A\\mu')\\\\\n           &= \\tr(A\\Sigma) + \\mu'A\\mu\n\\end{align}\n</details>\n\n\n::: {#thm-saber_lee}\n## Saber and Lee - Teorema 3.1, p. 40\n\nSuponha que $X$ é uma matriz $n \\times p$ de posto $p$, tal que $H = X(X'X)^{-1}X'$. Então,\n\n  1. $H$ e $I - H$ são simétricas e idempotentes;\n  1. $\\posto(I -H) = \\tr(I - H) = n - p$;\n  1. $HX = X$.\n\n:::\n:::\n\n\nQueremos mostrar que $\\mathbb E(s^2) = \\displaystyle\\mathbb E\\left(\\frac{\\hat{u}'\\hat{u}}{n-k-1}\\right) = \\sigma^2$.\n\nSeja $H = X(X'X)^{-1}X'$ e $\\bs{M} = I - H$, então, $\\hat{u} = Y - \\hat Y = (I - H)Y = MY$. Além disso, sabemos que $\\bs{M}$ é simétrica e idempotente, como provado na Questão 1. Assim,\n\n$$\\bbE(\\hat{u}'\\hat{u}) = \\bbE(Y'M'MY) = \\bbE(Y'MMY) =\\bbE(Y'MY)$$\n\nEntão, pelo @thm-esp_forma_quadratica e @thm-saber_lee, temos\n\\begin{align*}\n\\bbE(Y'MY) &= \\tr(M\\sigma^2I) + (X\\beta)'MX\\beta\\\\\n           &= \\sigma^2 \\tr(M) + \\beta'X'(I - H)X\\beta\\\\\n           &= \\sigma^2 \\tr(I - H) + \\beta'X'X\\beta -\n                \\beta'X'HX\\beta\\\\\n           &= \\sigma^2 (n - (k + 1)) + \\beta'X'X\\beta -\n                \\beta'X'X\\beta\\\\\n           &= \\sigma^2(n - k - 1)\n\\end{align*}\n\n\nPortanto,\n$$\n\\bbE(s^2) = \\displaystyle\\bbE \\left( \\frac{\\hat{u}'\\hat{u}}{n-k-1} \\right) = \\frac{1}{n-k-1}\\bbE (Y'MY) = \\frac{\\sigma^2(n-k-1)}{n-k-1} = \\sigma^2\n$$\n\n<br><br>\n\n\n::: {.callout-note icon=false}\n# Questão 3\n\nMostre que $R^2$ nunca diminui quando incluimos novas variáveis no modelo.\n:::\n\nConsidere o modelo\n\n$$ \nY = X\\beta + u\n$$ {#eq-Q3_mod1}\nAlém disso, considere adicionar uma ou mais novas variáveis ao modelo @eq-Q3_mod1, então, temos que:\n\n$$\nY = X_0 \\beta_0 + X \\beta_1 + v\n$$ {#eq-Q3_mod2}\n\nCalculando a $SQE$ do modelo da @eq-Q3_mod2:\n\n\\begin{align*}\n\\hat v' \\hat v = (Y - \\hat Y)'(Y - \\hat Y)\\\\\n      &= (X\\hat\\beta + \\hat u - X_0 \\hat\\beta_0 - X\\hat\\beta_1)'(X\\hat\\beta + \\hat u - X_0 \\hat\\beta_0 - X\\hat\\beta_1)\\\\\n      &= (X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0 + \\hat u)'(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0 + \\hat u)\\\\\n      &= \\underbrace{(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0)'(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0)}_{A} + 2 \\hat u'(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0) + \\hat u' \\hat u\\\\\n      & = A + 2 \\hat u'X (\\beta - \\hat\\beta_1) - 2\\hat u' X_0\\hat\\beta_0 + \\hat u' \\hat u\n\\end{align*}\n\nComo provado na Lista 2, desde que $\\beta$ seja estimado por OLS, então, $\\hat u'X = \\hat u'X_0 = 0'$. Logo,\n\nLogo,\n$$\\hat v' \\hat v = A + \\hat u' \\hat u$$\n\nVeja que $A = (X (\\hat\\beta - \\hat\\beta_1) - X_0 \\hat\\beta_0)^2_2 \\Rightarrow A \\geq 0$. Portanto,\n\n$$\n\\hat v' \\hat v \\leq \\hat u' \\hat u\n$$\n\nComo $R^2 = 1 - \\frac{SQE}{SQT}$, temos\n\n$$1 - \\frac{\\hat v' \\hat v}{SQT} \\geq 1 - \\frac{\\hat u' \\hat u}{SQT}$$\n\nPortanto, adicionar preditores, não diminuirá o $R^2$.\n\n<br><br>\n\n\n::: {.callout-note icon=false}\n# Questão 4\n\nUtilize o *dataset* `htv`, estime o modelo de regressão\n$$educ = \\beta_0 + \\beta_1 motheduc + \\beta_2fatheduc + \\beta_3abil + \\beta_4 abil^2 + u$$\ne interprete os resultados.\n:::\n\n\n\n\n\n::: {.panel-tabset}\n\n## R\n\n::: {.cell}\n\n```{.r .cell-code}\n# package e banco de dados\nrequire(wooldridge)\ndata(htv)\n\nfit_r <- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), data=htv)\ncoef(fit_r)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n(Intercept)    motheduc    fatheduc        abil   I(abil^2) \n 8.24022643  0.19012613  0.10893866  0.40146240  0.05059901 \n```\n\n\n:::\n:::\n\n\n\n## Python\n\n::: {.cell}\n\n```{.python .cell-code}\n# packages\nimport statsmodels.formula.api as smf\nimport wooldridge as woo\n\n# lendo dados\nhtv = woo.dataWoo('htv')\nhtv = r['htv']  # leitura alternativa (direto do r)\n\n# ajustando modelo\nfit_py = smf.ols(formula='educ ~ motheduc + fatheduc + abil + I(abil**2)',\n                 data=htv).fit().params\nfit_py\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nIntercept       8.240226\nmotheduc        0.190126\nfatheduc        0.108939\nabil            0.401462\nI(abil ** 2)    0.050599\ndtype: float64\n```\n\n\n:::\n:::\n\n\n\n## Julia\n\n::: {.cell}\n\n```{.julia .cell-code}\n# packages\nusing WooldridgeDatasets, DataFrames, Statistics, GLM\n\n# dados\nhtv = DataFrame(wooldridge(\"htv\"));\n\nfit_julia = lm(@formula(educ ~ motheduc + fatheduc + abil + abil^2), htv);\ncoeficientes = DataFrame(Beta = coefnames(fit_julia), Valor = coef(fit_julia))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n5×2 DataFrame\n Row │ Beta         Valor\n     │ String       Float64\n─────┼───────────────────────\n   1 │ (Intercept)  8.24023\n   2 │ motheduc     0.190126\n   3 │ fatheduc     0.108939\n   4 │ abil         0.401462\n   5 │ abil ^ 2     0.050599\n```\n\n\n:::\n:::\n\n\n:::\n\n<br><br>\n\n\n::: {.callout-note icon=false}\n# Questão 5\n\nPove o Teorema FWL.\n\n\n::: {#thm-fwl}\n## Frisch-Waugh-Lovell\n\nSejam os modelos\n$$\nY = X_1 \\beta_1 + X_2\\beta_2 + {u} \\qquad \\text{e} \\qquad M_1Y = M_1 X_2 \\beta_2 + \\nu\n$$\nem que $M_1 = I - X_1 (X_1' X_1)^{-1}X_1'$. Então,\n\n1. $\\hat\\beta_2$ em ambas regressões são numericamente idênticos;\n1. $\\hat u$ e $\\hat \\nu$ são numericamente idênticos.\n:::\n:::\n\nConsidere o modelo\n\n$$\nY = X\\beta + u\n$$ {#eq-modelo}\nonde $X$ é uma matriz $n \\times p$, $\\beta$ é um vetor de dimensão $k \\times 1$ e $u$ é o vetor do erro aleatório de dimensão $n \\times 1$. Note que $X$ e $\\beta$ podem ser escritos como:\n\n$$\nX = \\begin{bmatrix}X_1 & X_2\\end{bmatrix} \\hspace{1cm} \\text{e} \\hspace{1cm} \\beta = \\begin{bmatrix} \\beta_1\\\\ \\beta_2 \\end{bmatrix}\n$$\n\nonde $X_1$ é uma matriz $n \\times k_1$, $X_2$ é uma matriz $n \\times (p - k_1)$, $\\beta_1$ é um vetor de dimensão $k_1 \\times 1$ e $\\beta_2$ é um vetor de dimensão $(k - k_1) \\times 1$. Então, o modelo da @eq-modelo pode ser escrito como\n\n$$\nY = X_1\\beta_1 + X_2\\beta_2 + u \n$$\n\nVamos provar que $\\hat \\nu$ e $\\hat u$ são numericamente identicos. Para isso considere\n\n$$\nY = X_1 \\hat\\beta_1 + X_2 \\hat\\beta_2 + \\hat u \n$$ {#eq-obs}\n\nAlém disso, sabemos (da Lista 2) que $X' \\hat u = \\bar 0$, então\n$$\n\\begin{bmatrix} X_1' \\\\ X_2' \\end{bmatrix} \\hat u = \\begin{bmatrix}\\bar0 \\\\ \\bar0 \\end{bmatrix} \\Rightarrow X_1' \\hat u = X_2' \\hat u = \\bar0 \\tag{5} \\label{eq-lista2}\n$$\n\nMultiplicando a @eq-obs por $M_1$, temos\n\n\\begin{align*}\nM_1 Y &= M_1X_1\\hat\\beta_1 + M_1X_2\\hat\\beta_2 + M_1\\hat u\\\\\n      &= (I - X_1(X_1' X_1)^{-1}X_1')X_1\\hat\\beta_1 + M_1X_2\\hat\\beta_2 + (I - X_1(X_1' X_1)^{-1}X_1')\\hat u\\\\\n      &= (X_1 - X_1 (X_1' X_1)^{-1}X_1'X_1) \\beta_1 + M_1X_2\\hat\\beta_2 + \\hat u - X_1(X_1' X_1)^{-1} \\underbrace{X_1'\\hat u}_{\\begin{array}{c}\\bar0\\\\\\text{(por \\eqref{eq-lista2})}\\end{array}}\\\\\n      &= M_1X_2\\hat\\beta_2 + \\hat u\n\\end{align*}\n\nLogo,\n\n$$M_1 Y = M_1X_2\\hat\\beta_2 + \\hat \\nu$$\ncom $\\hat\\nu = \\hat u$. Portanto, fica provado que $\\hat\\nu$ e $\\hat u$ são numericamente identicos.\n\nAlém disso, note que $\\hat\\beta_2$ não foi alterado, e foi possível chegar em um modelo a partir, logo, $\\hat\\beta_2$ é numericamente idêntico em ambos modelos.\n\n<br><br>\n\n\n# Referências\n\nSaber A.F.G.; Lee, A.J. (2003). *Linear Regression Analysis*. Segunda edição. Wiley.\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}