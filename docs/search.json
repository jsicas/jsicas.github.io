[
  {
    "objectID": "listas_econometria/Lista3_Resolucao_ME715.html",
    "href": "listas_econometria/Lista3_Resolucao_ME715.html",
    "title": "Lista 3 - Econometria - ME715",
    "section": "",
    "text": "\\[\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\posto}{posto}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\Var}{\\mathbb{V}}\n\\DeclareMathOperator{\\bbE}{\\mathbb{E}}\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\]\n\n\n\n\n\n\n\nQuestão 1\n\n\n\nProve que \\(\\boldsymbol M = I - X(X'X)^{-1}X'\\) é simétrica e idempotente.\n\n\nVamos provar que \\(\\bs{M}\\) é simétrica, isto é, \\(\\bs{M}' = \\bs{M}\\):\n\\[\n\\bs{M}' = (I - X(X'X)^{-1}X')' = I - X(X'X)^{-1}X' = \\bs{M}\n\\]\nVamos provar que \\(\\bs{M}\\) é idempotente, isto é, \\(\\bs{M}\\bs{M} = \\bs{M}\\):\n\\[\\begin{align*}\n  \\bs{M}\\bs{M} &= (I - X(X'X)^{-1}X') (I - X(X'X)^{-1}X')\\\\\n  &= I - X(X'X)^{-1}X' -  X(X'X)^{-1}X' + X(X'X)^{-1}X' X(X'X)^{-1}X'\\\\\n  &= I - 2 X(X'X)^{-1}X' + X(X'X)^{-1}X'\\\\\n  &= I - X(X'X)^{-1}X'\\\\\n  &= \\bs{M}\n\\end{align*}\\]\nPortanto, fica provado que \\(\\bs{M}\\) é simétrica e idempotente.\n\n\n\n\n\n\n\nQuestão 2\n\n\n\nMostre que \\(s^2 = \\displaystyle\\frac{\\hat u' \\hat u}{n-k-1}\\) é um estimador não viesado para \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\nResultados auxiliares\n\n\n\n\n\n\nTeorema 1 (Esperança de forma quadrática) Seja \\(X\\) um vetor aleatório \\(n\\times 1\\) com média \\(\\mu\\) e covariância \\(\\Sigma\\) e \\(A \\in \\mathbb{R}^{n \\times n}\\) uma matriz simétrica. Então, a esperança de uma forma quadrática \\(X'AX\\) é dada por: \\[\\bbE(X'AX) = \\tr(A\\Sigma) + \\mu'A\\mu\\]\n\n\n\nProof\n\nNote que \\(X'AX\\) é uma matriz \\(1 \\times 1\\), então \\[\n\\mathbb E(X'AX) = \\bbE(\\tr(X'AX)) = \\bbE(\\tr(AXX')) = \\tr(A\\bbE(XX'))\n\\]\nAlém disso, como \\(\\Var(X) = \\bbE(XX') - \\bbE(X)\\bbE(X)' \\Rightarrow \\bbE(XX') = \\Var(X) + \\bbE(X)\\bbE(X)'\\), então\n\\[\\begin{align}\n\\bbE(X'AX) &= \\tr(A\\bbE(XX'))\\\\\n           &= \\tr(A[\\Var(X) + \\bbE(X)\\bbE(X)'])\\\\\n           &= \\tr(A(\\Sigma + \\mu\\mu'))\\\\\n           &= \\tr(A\\Sigma) + \\tr(A\\mu\\mu')\\\\\n           &= \\tr(A\\Sigma) + \\tr(\\mu'A\\mu')\\\\\n           &= \\tr(A\\Sigma) + \\mu'A\\mu\n\\end{align}\\]\n\n\nTeorema 2 (Saber and Lee - Teorema 3.1, p. 40) Suponha que \\(X\\) é uma matriz \\(n \\times p\\) de posto \\(p\\), tal que \\(H = X(X'X)^{-1}X'\\). Então,\n\n\\(H\\) e \\(I - H\\) são simétricas e idempotentes;\n\\(\\posto(I -H) = \\tr(I - H) = n - p\\);\n\\(HX = X\\).\n\n\n\n\n\nQueremos mostrar que \\(\\mathbb E(s^2) = \\displaystyle\\mathbb E\\left(\\frac{\\hat{u}'\\hat{u}}{n-k-1}\\right) = \\sigma^2\\).\nSeja \\(H = X(X'X)^{-1}X'\\) e \\(\\bs{M} = I - H\\), então, \\(\\hat{u} = Y - \\hat Y = (I - H)Y = MY\\). Além disso, sabemos que \\(\\bs{M}\\) é simétrica e idempotente, como provado na Questão 1. Assim,\n\\[\\bbE(\\hat{u}'\\hat{u}) = \\bbE(Y'M'MY) = \\bbE(Y'MMY) =\\bbE(Y'MY)\\]\nEntão, pelo Teorema 1 e Teorema 2, temos \\[\\begin{align*}\n\\bbE(Y'MY) &= \\tr(M\\sigma^2I) + (X\\beta)'MX\\beta\\\\\n           &= \\sigma^2 \\tr(M) + \\beta'X'(I - H)X\\beta\\\\\n           &= \\sigma^2 \\tr(I - H) + \\beta'X'X\\beta -\n                \\beta'X'HX\\beta\\\\\n           &= \\sigma^2 (n - (k + 1)) + \\beta'X'X\\beta -\n                \\beta'X'X\\beta\\\\\n           &= \\sigma^2(n - k - 1)\n\\end{align*}\\]\nPortanto, \\[\n\\bbE(s^2) = \\displaystyle\\bbE \\left( \\frac{\\hat{u}'\\hat{u}}{n-k-1} \\right) = \\frac{1}{n-k-1}\\bbE (Y'MY) = \\frac{\\sigma^2(n-k-1)}{n-k-1} = \\sigma^2\n\\]\n\n\n\n\n\n\n\nQuestão 3\n\n\n\nMostre que \\(R^2\\) nunca diminui quando incluimos novas variáveis no modelo.\n\n\nConsidere o modelo\n\\[\nY = X\\beta + u\n\\tag{1}\\] Além disso, considere adicionar uma ou mais novas variáveis ao modelo Eq. 1, então, temos que:\n\\[\nY = X_0 \\beta_0 + X \\beta_1 + v\n\\tag{2}\\]\nCalculando a \\(SQE\\) do modelo da Eq. 2:\n\\[\\begin{align*}\n\\hat v' \\hat v = (Y - \\hat Y)'(Y - \\hat Y)\\\\\n      &= (X\\hat\\beta + \\hat u - X_0 \\hat\\beta_0 - X\\hat\\beta_1)'(X\\hat\\beta + \\hat u - X_0 \\hat\\beta_0 - X\\hat\\beta_1)\\\\\n      &= (X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0 + \\hat u)'(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0 + \\hat u)\\\\\n      &= \\underbrace{(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0)'(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0)}_{A} + 2 \\hat u'(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0) + \\hat u' \\hat u\\\\\n      & = A + 2 \\hat u'X (\\beta - \\hat\\beta_1) - 2\\hat u' X_0\\hat\\beta_0 + \\hat u' \\hat u\n\\end{align*}\\]\nComo provado na Lista 2, desde que \\(\\beta\\) seja estimado por OLS, então, \\(\\hat u'X = \\hat u'X_0 = 0'\\). Logo,\nLogo, \\[\\hat v' \\hat v = A + \\hat u' \\hat u\\]\nVeja que \\(A = (X (\\hat\\beta - \\hat\\beta_1) - X_0 \\hat\\beta_0)^2_2 \\Rightarrow A \\geq 0\\). Portanto,\n\\[\n\\hat v' \\hat v \\leq \\hat u' \\hat u\n\\]\nComo \\(R^2 = 1 - \\frac{SQE}{SQT}\\), temos\n\\[1 - \\frac{\\hat v' \\hat v}{SQT} \\geq 1 - \\frac{\\hat u' \\hat u}{SQT}\\]\nPortanto, adicionar preditores, não diminuirá o \\(R^2\\).\n\n\n\n\n\n\n\nQuestão 4\n\n\n\nUtilize o dataset htv, estime o modelo de regressão \\[educ = \\beta_0 + \\beta_1 motheduc + \\beta_2fatheduc + \\beta_3abil + \\beta_4 abil^2 + u\\] e interprete os resultados.\n\n\n\nRPythonJulia\n\n\n\n# package e banco de dados\nrequire(wooldridge)\ndata(htv)\n\nfit_r &lt;- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), data=htv)\ncoef(fit_r)\n\n(Intercept)    motheduc    fatheduc        abil   I(abil^2) \n 8.24022643  0.19012613  0.10893866  0.40146240  0.05059901 \n\n\n\n\n\n# packages\nimport statsmodels.formula.api as smf\nimport wooldridge as woo\n\n# lendo dados\nhtv = woo.dataWoo('htv')\nhtv = r['htv']  # leitura alternativa (direto do r)\n\n# ajustando modelo\nfit_py = smf.ols(formula='educ ~ motheduc + fatheduc + abil + I(abil**2)',\n                 data=htv).fit().params\nfit_py\n\nIntercept       8.240226\nmotheduc        0.190126\nfatheduc        0.108939\nabil            0.401462\nI(abil ** 2)    0.050599\ndtype: float64\n\n\n\n\n\n# packages\nusing WooldridgeDatasets, DataFrames, Statistics, GLM\n\n# dados\nhtv = DataFrame(wooldridge(\"htv\"));\n\nfit_julia = lm(@formula(educ ~ motheduc + fatheduc + abil + abil^2), htv);\ncoeficientes = DataFrame(Beta = coefnames(fit_julia), Valor = coef(fit_julia))\n\n5×2 DataFrame\n Row │ Beta         Valor\n     │ String       Float64\n─────┼───────────────────────\n   1 │ (Intercept)  8.24023\n   2 │ motheduc     0.190126\n   3 │ fatheduc     0.108939\n   4 │ abil         0.401462\n   5 │ abil ^ 2     0.050599\n\n\n\n\n\n\n\n\n\n\n\n\nQuestão 5\n\n\n\nPove o Teorema FWL.\n\nTeorema 3 (Frisch-Waugh-Lovell) Sejam os modelos \\[\nY = X_1 \\beta_1 + X_2\\beta_2 + {u} \\qquad \\text{e} \\qquad M_1Y = M_1 X_2 \\beta_2 + \\nu\n\\] em que \\(M_1 = I - X_1 (X_1' X_1)^{-1}X_1'\\). Então,\n\n\\(\\hat\\beta_2\\) em ambas regressões são numericamente idênticos;\n\\(\\hat u\\) e \\(\\hat \\nu\\) são numericamente idênticos.\n\n\n\n\nConsidere o modelo\n\\[\nY = X\\beta + u\n\\tag{3}\\] onde \\(X\\) é uma matriz \\(n \\times p\\), \\(\\beta\\) é um vetor de dimensão \\(k \\times 1\\) e \\(u\\) é o vetor do erro aleatório de dimensão \\(n \\times 1\\). Note que \\(X\\) e \\(\\beta\\) podem ser escritos como:\n\\[\nX = \\begin{bmatrix}X_1 & X_2\\end{bmatrix} \\hspace{1cm} \\text{e} \\hspace{1cm} \\beta = \\begin{bmatrix} \\beta_1\\\\ \\beta_2 \\end{bmatrix}\n\\]\nonde \\(X_1\\) é uma matriz \\(n \\times k_1\\), \\(X_2\\) é uma matriz \\(n \\times (p - k_1)\\), \\(\\beta_1\\) é um vetor de dimensão \\(k_1 \\times 1\\) e \\(\\beta_2\\) é um vetor de dimensão \\((k - k_1) \\times 1\\). Então, o modelo da Eq. 3 pode ser escrito como\n\\[\nY = X_1\\beta_1 + X_2\\beta_2 + u\n\\]\nVamos provar que \\(\\hat \\nu\\) e \\(\\hat u\\) são numericamente identicos. Para isso considere\n\\[\nY = X_1 \\hat\\beta_1 + X_2 \\hat\\beta_2 + \\hat u\n\\tag{4}\\]\nAlém disso, sabemos (da Lista 2) que \\(X' \\hat u = \\bar 0\\), então \\[\n\\begin{bmatrix} X_1' \\\\ X_2' \\end{bmatrix} \\hat u = \\begin{bmatrix}\\bar0 \\\\ \\bar0 \\end{bmatrix} \\Rightarrow X_1' \\hat u = X_2' \\hat u = \\bar0 \\tag{5} \\label{eq-lista2}\n\\]\nMultiplicando a Eq. 4 por \\(M_1\\), temos\n\\[\\begin{align*}\nM_1 Y &= M_1X_1\\hat\\beta_1 + M_1X_2\\hat\\beta_2 + M_1\\hat u\\\\\n      &= (I - X_1(X_1' X_1)^{-1}X_1')X_1\\hat\\beta_1 + M_1X_2\\hat\\beta_2 + (I - X_1(X_1' X_1)^{-1}X_1')\\hat u\\\\\n      &= (X_1 - X_1 (X_1' X_1)^{-1}X_1'X_1) \\beta_1 + M_1X_2\\hat\\beta_2 + \\hat u - X_1(X_1' X_1)^{-1} \\underbrace{X_1'\\hat u}_{\\begin{array}{c}\\bar0\\\\\\text{(por \\eqref{eq-lista2})}\\end{array}}\\\\\n      &= M_1X_2\\hat\\beta_2 + \\hat u\n\\end{align*}\\]\nLogo,\n\\[M_1 Y = M_1X_2\\hat\\beta_2 + \\hat \\nu\\] com \\(\\hat\\nu = \\hat u\\). Portanto, fica provado que \\(\\hat\\nu\\) e \\(\\hat u\\) são numericamente identicos.\nAlém disso, note que \\(\\hat\\beta_2\\) não foi alterado, e foi possível chegar em um modelo a partir, logo, \\(\\hat\\beta_2\\) é numericamente idêntico em ambos modelos.\n\n\nReferências\nSaber A.F.G.; Lee, A.J. (2003). Linear Regression Analysis. Segunda edição. Wiley."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jsicas.github.io",
    "section": "",
    "text": "Link da Apresentação ME861"
  },
  {
    "objectID": "scripts/ic/funcionais_agregados.html",
    "href": "scripts/ic/funcionais_agregados.html",
    "title": "Dados Funcionais Agregados",
    "section": "",
    "text": "\\[\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\posto}{posto}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\p}{\\mathbb{P}}\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\newcommand{\\bm}[1]{\\mathbf{#1}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\NN}{\\mathbb{N}}\n\\newcommand{\\norm}{\\mathcal{N}}\n\\]"
  },
  {
    "objectID": "scripts/ic/funcionais_agregados.html#dados-simulados",
    "href": "scripts/ic/funcionais_agregados.html#dados-simulados",
    "title": "Dados Funcionais Agregados",
    "section": "Dados Simulados",
    "text": "Dados Simulados\nForam utilizadas as funções Bumps e Doppler com \\(SNR=3\\) e \\(5\\) considerando \\(1024\\) pontos por curva.\n\n\nCode\n######## packages e configurações ########\nset.seed(282829)\nsetwd('C:/Users/jvsiq/Desktop/Estudo e Afins/IC - FAPESP/codigo')\n\nrequire(wavethresh)\nrequire(knitr)\nrequire(ic)\n\n######## Funções ########\nbumps &lt;- f_test()$bumps\ndoppler &lt;- f_test()$doppler\npar(mfrow=c(1,2))\nplot(bumps, type='l', main='Bumps')\nplot(doppler, type='l', main='Doppler'); par(mfrow=c(1,1))\n\n\n\n\n\n\n\n\n\n\n\\(SNR=3\\)\n\n\nCode\n# gerando banco de dados\ny1 &lt;- runif(10)  # pesos da curva 1\ny2 &lt;- 1 - y1     # pesos da curva 2\n\ny &lt;- matrix(c(y1, y2), nrow=2, byrow=T)  # pesos\nfun_agr &lt;- matrix(0, length(y1), 1024)   # observacoes\n\n# gerando amostra com snr=3\nfor (i in 1:length(y1)) fun_agr[i,] &lt;- y1[i]*bumps + y2[i]*doppler + rnorm(1024, 0, 7/3)\nplot(1, main='Amostra',xlim=c(0,1010),ylim=c(-15, 58),type='n',xlab='',ylab='')\nfor (i in 1:length(y1)) lines(fun_agr[i,], col=i)\n\n\n\n\n\n\n\n\n\n\nFrequentista\n\n\nCode\n# dominio das ondaletas\nD &lt;- apply(fun_agr, MARGIN=1, wd)                 # coeficientes empiricos\nD_shrink &lt;- sapply(D, function(x = threshold(D)) c(accessC(x, level=0), x$D))\ngamma &lt;- D_shrink %*% t(y) %*% solve(y %*% t(y))  # coeficientes de ondaletas estimados\nalpha &lt;- GenW(n=1024) %*% gamma                   # funções recuperadas\n\n# funções recuperadas\nplot(alpha[,1], type='l', col='blue', main='Bumps Recuperada'); lines(bumps, type='l')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha[,2], type='l', col='blue', main='Doppler Recuperada'); lines(doppler, type='l')\n\n\n\n\n\n\n\n\n\nCode\n# calculando erro\nMSE_1 &lt;- sum((alpha[,1] - bumps)^2) / length(bumps)     # MSE da fç componente 1 (bumps)\nMSE_2 &lt;- sum((alpha[,2] - doppler)^2) / length(doppler) # MSE da fç componente 2 (doppler)\nAMSE &lt;- (MSE_1 + MSE_2) / 2\n\nkable(data.frame(MSE_1, MSE_2, AMSE))\n\n\n\n\n\nMSE_1\nMSE_2\nAMSE\n\n\n\n\n2.372409\n1.765951\n2.06918\n\n\n\n\n\n\n\nBayesiano\n\n\nCode\n# dominio das ondaletas\nD_shrink_bayes &lt;- sapply(D, function(x = D) c(accessC(x, lev=0),\n                            logis_shrink(x$D, 0.8, 1, 2)))\ngamma_bayes &lt;- D_shrink_bayes %*% t(y) %*% solve(y %*% t(y))  # coefs de ondaletas estimados\nalpha_bayes &lt;- GenW(n=1024) %*% gamma_bayes                   # funções recuperadas\n\n# funcoes recuperadas pelo metodo bayesiano\nplot(alpha_bayes[,1], type='l', col='blue', main='Bumps Recuperada'); lines(bumps, type='l')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha_bayes[,2], type='l', col='blue', main='Doppler Recuperada'); lines(doppler, type='l')\n\n\n\n\n\n\n\n\n\nCode\n# calculando erro\nMSE_bayes_1 &lt;- sum((alpha_bayes[,1] - bumps)^2) / length(bumps)     # MSE da fç componente 1 (bumps)\nMSE_bayes_2 &lt;- sum((alpha_bayes[,2] - doppler)^2) / length(doppler) # MSE da fç componente 2 (doppler)\nAMSE_bayes &lt;- (MSE_bayes_1 + MSE_bayes_1) / 2\n\nkable(data.frame(MSE_bayes_1, MSE_bayes_2, AMSE_bayes))\n\n\n\n\n\nMSE_bayes_1\nMSE_bayes_2\nAMSE_bayes\n\n\n\n\n1.676771\n1.195455\n1.676771\n\n\n\n\n\n\n\nFrequentista X Bayesiano\n\n\nCode\n# comparacao entre funcoes recuperadas pelos 2 metodos\nplot(alpha[,1], type='l', col='red');lines(alpha_bayes[,1], col='blue');lines(bumps)\nlegend('topright', legend=c('Verdadeiro', 'Bayes', 'Freq'),\n       col=c('black', 'blue', 'red'), lwd=2, bty='n')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha[,2], type='l', col='red');lines(alpha_bayes[,2], col='blue');lines(doppler)\nlegend('topright', legend=c('Verdadeiro', 'Bayes', 'Freq'),\n       col=c('black', 'blue', 'red'), lwd=2, bty='n')\n\n\n\n\n\n\n\n\n\nCode\nkable(data.frame('Medida'=c('Bumps', 'Doppler', 'AMSE'),\n                 'Frequentista'=c(MSE_1, MSE_2, AMSE),\n                 'Bayesiano'=c(MSE_bayes_1, MSE_bayes_2, AMSE_bayes)))\n\n\n\n\n\nMedida\nFrequentista\nBayesiano\n\n\n\n\nBumps\n2.372409\n1.676771\n\n\nDoppler\n1.765951\n1.195455\n\n\nAMSE\n2.069180\n1.676771\n\n\n\n\n\n\n\n\n\\(SNR=5\\)\n\nFrequentista\n\n\nCode\n# gerando banco de dados\ny1 &lt;- runif(10)  # pesos da curva 1\ny2 &lt;- 1 - y1     # pesos da curva 2\n\ny &lt;- matrix(c(y1, y2), nrow=2, byrow=T)  # pesos\nfun_agr &lt;- matrix(0, length(y1), 1024)   # observacoes\n\n# gerando amostra com snr=5\nfor (i in 1:length(y1)) fun_agr[i,] &lt;- y1[i]*bumps + y2[i]*doppler + rnorm(1024, 0, 7/5)\nplot(1, main='Amostra',xlim=c(0,1010),ylim=c(-15, 57),type='n',xlab='',ylab='')\nfor (i in 1:length(y1)) lines(fun_agr[i,], col=i)\n\n\n\n\n\n\n\n\n\nCode\n# dominio das ondaletas\nD &lt;- apply(fun_agr, MARGIN=1, wd)                 # coeficientes empiricos\nD_shrink &lt;- sapply(D, function(x = threshold(D)) c(accessC(x, level=0), x$D))\ngamma &lt;- D_shrink %*% t(y) %*% solve(y %*% t(y))  # coeficientes de ondaletas estimados\nalpha &lt;- GenW(n=1024) %*% gamma                   # funções recuperadas\n\n# funções recuperadas\nplot(alpha[,1], type='l', col='blue', main='Bumps Recuperada'); lines(bumps, type='l')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha[,2], type='l', col='blue', main='Doppler Recuperada'); lines(doppler, type='l')\n\n\n\n\n\n\n\n\n\n\n\nBayesiano\n\n\nCode\n# dominio das ondaletas\nD_shrink_bayes &lt;- sapply(D, function(x = D) c(accessC(x, lev=0),\n                            logis_shrink(x$D, 0.8, 1, 2)))\ngamma_bayes &lt;- D_shrink_bayes %*% t(y) %*% solve(y %*% t(y))  # coefs de ondaletas estimados\nalpha_bayes &lt;- GenW(n=1024) %*% gamma_bayes                   # funções recuperadas\n\n# funcoes recuperadas pelo metodo bayesiano\nplot(alpha_bayes[,1], type='l', col='blue', main='Bumps Recuperada'); lines(bumps, type='l')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha_bayes[,2], type='l', col='blue', main='Doppler Recuperada'); lines(doppler, type='l')\n\n\n\n\n\n\n\n\n\nCode\n# calculando erro\nMSE_bayes_1 &lt;- sum((alpha_bayes[,1] - bumps)^2) / length(bumps)     # MSE da fç componente 1 (bumps)\nMSE_bayes_2 &lt;- sum((alpha_bayes[,2] - doppler)^2) / length(doppler) # MSE da fç componente 2 (doppler)\nAMSE_bayes &lt;- (MSE_bayes_1 + MSE_bayes_1) / 2\n\nkable(data.frame(MSE_bayes_1, MSE_bayes_2, AMSE_bayes))\n\n\n\n\n\nMSE_bayes_1\nMSE_bayes_2\nAMSE_bayes\n\n\n\n\n0.2936653\n0.6326009\n0.2936653\n\n\n\n\n\n\n\nFrequentista X Bayesiano\n\n\nCode\n# comparacao entre funcoes recuperadas pelos 2 metodos\nplot(alpha[,1], type='l', col='red');lines(alpha_bayes[,1], col='blue');lines(bumps)\nlegend('topright', legend=c('Verdadeiro', 'Bayes', 'Freq'),\n       col=c('black', 'blue', 'red'), lwd=2, bty='n')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha[,2], type='l', col='red');lines(alpha_bayes[,2], col='blue');lines(doppler)\nlegend('topright', legend=c('Verdadeiro', 'Bayes', 'Freq'),\n       col=c('black', 'blue', 'red'), lwd=2, bty='n')\n\n\n\n\n\n\n\n\n\nCode\nkable(data.frame('Medida'=c('Bumps', 'Doppler', 'AMSE'),\n                 'Frequentista'=c(MSE_1, MSE_2, AMSE),\n                 'Bayesiano'=c(MSE_bayes_1, MSE_bayes_2, AMSE_bayes)))\n\n\n\n\n\nMedida\nFrequentista\nBayesiano\n\n\n\n\nBumps\n2.372409\n0.2936653\n\n\nDoppler\n1.765951\n0.6326009\n\n\nAMSE\n2.069180\n0.2936653"
  },
  {
    "objectID": "scripts/ic/funcionais_agregados.html#tecator",
    "href": "scripts/ic/funcionais_agregados.html#tecator",
    "title": "Dados Funcionais Agregados",
    "section": "Tecator",
    "text": "Tecator\nConjunto de dados que contém a espectrometria da carne. Foi necessário transformar o banco de dados para um conjunto de vetores diádicos.\n\n\nCode\nrequire(fda.usc)\ndata('tecator')\n\n# Tecator\nfun_tec &lt;- tecator$absorp.fdata[,20:83]\npesos &lt;- t(tecator$y)\n\nplot(fun_tec)\n\n\n\n\n\n\n\n\n\nCode\nD &lt;- apply(fun_tec$data, MARGIN=1, wd)\nD_shrink &lt;- sapply(D, function(x = threshold(D)) c(accessC(x, level=0), x$D))\ngamma &lt;- D_shrink %*% t(pesos) %*% solve(pesos %*% t(pesos))\nalpha &lt;- GenW(n=64) %*% gamma\n\n# funções recuperadas tecator\nplot(alpha[,1], type='l', col='blue', main='Gordura')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha[,2], type='l', col='blue', main='Água')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha[,3], type='l', col='blue', main='Proteina')\n\n\n\n\n\n\n\n\n\n\n\nCode\n# utilizando alpha=0.5\ncurve(logis_shrink(x, 0.5, 0.25, 1), -3, 3, n=500)\nD &lt;- apply(fun_tec$data, MARGIN=1, wd)  # DWT\nD_shrink_bayes_5 &lt;- sapply(D, function(x = D) c(accessC(x, level=0),\n                                              logis_shrink(x$D,0.5,0.25,1)))\ngamma_bayes_5 &lt;- D_shrink_bayes_5 %*% t(pesos) %*% solve(pesos %*% t(pesos))\nalpha_bayes_5 &lt;- GenW(n=64) %*% gamma_bayes_5\n\n# utilizando alpha=0.85\ncurve(logis_shrink(x, 0.85, 0.25, 1), -3, 3, n=500, add=T, col='blue')\n\n\n\n\n\n\n\n\n\nCode\nD &lt;- apply(fun_tec$data, MARGIN=1, wd)  # DWT\nD_shrink_bayes_85 &lt;- sapply(D, function(x = D) c(accessC(x, level=0),\n                                                 logis_shrink(x$D,0.85,0.25,1)))\ngamma_bayes_85 &lt;- D_shrink_bayes_85 %*% t(pesos) %*% solve(pesos %*% t(pesos))\nalpha_bayes_85 &lt;- GenW(n=64) %*% gamma_bayes_85\n\n# Comparacao\nplot(alpha[,1], type='l', main='Gordura', col='red', ylim=c(0.039, 0.051))\nlines(alpha_bayes_5[,1], col='blue'); lines(alpha_bayes_85[,1], col='darkgreen')\nlegend('bottomright', legend=c('Bayes 50', 'Bayes 85', 'Freq'),\n       col=c('red', 'blue', 'darkgreen'), lwd=2, bty='n')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha[,2], type='l', main='Água', col='red', ylim=c(0.0031, 0.0188))\nlines(alpha_bayes_5[,2], col='blue'); lines(alpha_bayes_85[,2], col='darkgreen')\nlegend('bottomright', legend=c('Bayes 50', 'Bayes 85', 'Freq'),\n       col=c('red', 'blue', 'darkgreen'), lwd=2, bty='n')\n\n\n\n\n\n\n\n\n\nCode\nplot(alpha[,3], type='l', main='Proteina', col='red', ylim=c(0.085, 0.114))\nlines(alpha_bayes_5[,3], col='blue'); lines(alpha_bayes_85[,3], col='darkgreen')\nlegend('topright', legend=c('Bayes 50', 'Bayes 85', 'Freq'),\n       col=c('red', 'blue', 'darkgreen'), lwd=2, bty='n')"
  },
  {
    "objectID": "listas_econometria/Lista4_Resolucao_ME715.html",
    "href": "listas_econometria/Lista4_Resolucao_ME715.html",
    "title": "Lista 4 - Econometria - ME715",
    "section": "",
    "text": "\\[\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\posto}{posto}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\p}{\\mathbb{P}}\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\norm}{\\mathcal{N}}\n\\]\n\n\n\n\n\n\n\nQuestão 1\n\n\n\nProve que se \\(X \\sim \\norm_p(\\mu, \\Sigma)\\), então \\((X - \\mu)'\\Sigma^{-1} (X - \\mu) \\sim \\chi^2_p\\)\n\n\n\n\n\n\n\n\nResultados auxiliares\n\n\n\n\n\n\nDefinição 1 (Distribuição Chi-quadrado) Seja \\(Z_i \\overset{i.i.d.}{\\sim} \\norm(0,1)\\), então dizemos que \\[\\sum_{i=1}^n Z_i^2 \\sim \\chi^2_n\\] segue uma distribuição Qui-quadrado com \\(n\\) graus de liberdade.\n\n\n\n\n\nTeorema 1 Seja \\(A\\) uma matriz simétrica, então seus autovalores são todos reas.\n\n\nTeorema 2 (Teorema Espectral) Seja \\(A\\) uma matriz quadrada \\(n \\times n\\) e simétrica com autovalores reais, então, temos \\(v^{(1)},\\dots,v^{(n)}\\) autovetores ortogonais. Além disso, seja \\(V=(v^{(1)}, \\dots, v^{(n)})\\) e \\(D=\\operatorname{diag}(\\lambda_1, \\dots, \\lambda_n)\\), então \\[A = VDV'\\]\n\n\n\n\nSeja \\(X \\sim \\norm_p(\\mu,\\Sigma)\\), então\n\\[\\E\\left[ \\Sigma^{-\\frac{1}{2}}(X - \\mu) \\right] = \\Sigma^{-\\frac{1}{2}}(\\E(X) - \\mu)\\] \\[\\V\\left[ \\Sigma^{-\\frac{1}{2}}(X - \\mu) \\right] = \\Sigma^{-\\frac{1}{2}} \\V(X) \\left( \\Sigma^{-\\frac{1}{2}} \\right)' = \\Sigma^{-\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}\\Sigma^{\\frac{1}{2}}\\Sigma^{-\\frac{1}{2}} = I\\] Note que a a notação \\(\\Sigma^{-\\frac{1}{2}}\\) faz sentido, por \\(\\Sigma\\) ser uma matriz simétrica (Teorema 2). Logo, \\(\\Sigma^{-\\frac{1}{2}} (X - \\mu) \\sim \\norm(\\bar0, I)\\). Então, pela Definição 1,\n\\[\\left[ \\Sigma^{-\\frac{1}{2}}(X - \\mu) \\right]' \\Sigma^{-\\frac{1}{2}}(X - \\mu) = (X - \\mu)\\Sigma^{-1}(X - \\mu) \\sim \\chi^2_p\\]\n\n\n\n\n\n\n\nQuestão 2\n\n\n\nSuponha que ajustou um modelo de regressão e obteve \\(\\beta_1 = 0.56\\) e um \\(\\text{p-valor} = 0.086\\) para testar \\(H_0∶ \\beta_1 = 0 \\;\\; V.S. \\;\\; H_1: \\beta_1 \\neq 0\\). Contudo, você está interessado em testar \\(H_0∶ \\beta_1 = 0 \\;\\; V.S. \\;\\; H_1: \\beta_1 &gt; 0\\), qual seria o p-valor? Rejeitaria \\(H_0\\) a um nível se significância de \\(5\\%\\)?\n\n\nIntuição:\n\n\n\n\n\n\n\n\n\nNote, então, que \\[\\p_{H_0} (|t| &gt; |t_{obs}|) = 2 \\p_{H_0} (t &gt; |t_{obs}|) = 2 \\p_{H_0} (t &lt; - |t_{obs}|)\\] Logo, o novo p-valor é de \\(\\frac{0.086}{2} = 0.043\\). Portanto, rejeito \\(H_0\\) a favor de \\(H_1\\), a um nível de \\(5\\%\\).\n\n\n\n\n\n\n\nQuestão 3\n\n\n\nUm erro comum em muito estudos aplicados é dizer que “aceitamos” \\(H_0\\). Contudo, nós, estatísticos, preferimos dizer “não rejeitamos” \\(H_0\\) e não “aceitamos” \\(H_0\\). Discuta o motivo.\n\n\nAo fazer um teste estatístico buscamos evidências contra \\(H_0\\) e em favor de \\(H_1\\), veja que isto não é equivalente, no caso de não encontrar evidência o suficiente, a falar que \\(H_0\\) é verdadeira.\n\n\n\n\n\n\n\nQuestão 4\n\n\n\nO dataset rdchem contém informação de 32 empresas da industria química. Entre as informações coletadas, temos: rdintens (gastos com pesquisa e desenvolvimento como uma porcentagem das vendas), sales (vendas mensuradas em mlhões de dólares) e profmarg (lucros como uma porcentagem das vendas). Ajuste um modelo da forma:\n\\[rdintens = \\beta_0 + \\beta_1 \\log(sales) + \\beta_2 profmarg + u\\] assumindo que as hipóteses do modelo linear clássico acontecem.\n\nInterprete os coeficientes de \\(\\log(sales)\\).\nTeste a hipóteses de que a intensidade de P&D não varia com sales contra a alternativa de que P&D aumenta com as vendas.\nInterprete o coeficiente de profmarg, ele é economicamente grande?\nprofmarg tem um efeito estatisticamente significativo sobre rdintens?\n\n\n\n\n\n\n\n\n\nResultados auxiliares\n\n\n\n\n\n\n\n\nTabela 1: Interpretação dos tipos de modelo\n\n\n\n\n\n\n\n\n\n\n\nModelo\nV. Dependente\nV. Independente\nInterpretação \\(\\beta_1\\)\n\n\n\n\nNível-Nível\n\\(y\\)\n\\(x\\)\n\\(\\Delta y = \\beta_1 \\Delta x\\)\n\n\nNível-Log\n\\(y\\)\n\\(\\log(x)\\)\n\\(\\Delta y = \\frac{\\beta_1}{100} \\Delta_\\% x\\)\n\n\nLog-Nível\n\\(\\log(y)\\)\n\\(x\\)\n\\(\\Delta_\\% y = 100\\beta_1 \\Delta x\\)\n\n\nLog-Log\n\\(\\log(y)\\)\n\\(\\log(x)\\)\n\\(\\Delta_\\% y = \\beta_1 \\Delta_\\% x\\)\n\n\n\n\n\n\n\n\n\n\n\nInterprete os coeficientes de \\(\\log(sales)\\).\n\n\nRPythonJulia\n\n\n\n# package e banco de dados\nrequire(wooldridge)\ndata(rdchem)\n\nfit_r &lt;- lm(rdintens ~ log(sales) + profmarg, data = rdchem)\nsummary(fit_r)\n\n\nCall:\nlm(formula = rdintens ~ log(sales) + profmarg, data = rdchem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.3016 -1.2707 -0.6895  0.8785  6.0369 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)  0.47225    1.67606   0.282    0.780\nlog(sales)   0.32135    0.21557   1.491    0.147\nprofmarg     0.05004    0.04578   1.093    0.283\n\nResidual standard error: 1.839 on 29 degrees of freedom\nMultiple R-squared:  0.09847,   Adjusted R-squared:  0.0363 \nF-statistic: 1.584 on 2 and 29 DF,  p-value: 0.2224\n\n\n\n\n\n# packages\nimport statsmodels.formula.api as smf\nimport wooldridge as woo\nimport numpy as np\n\n# lendo dados\nrdchem = woo.dataWoo('rdchem')\nrdchem = r['rdchem']  # leitura alternativa (direto do R)\n\n# ajustando modelo\nfit_py = smf.ols(formula='rdintens ~ np.log(sales) + profmarg', data=rdchem).fit()\nprint(fit_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               rdintens   R-squared:                       0.098\nModel:                            OLS   Adj. R-squared:                  0.036\nMethod:                 Least Squares   F-statistic:                     1.584\nDate:                qui, 24 out 2024   Prob (F-statistic):              0.222\nTime:                        01:15:22   Log-Likelihood:                -63.333\nNo. Observations:                  32   AIC:                             132.7\nDf Residuals:                      29   BIC:                             137.1\nDf Model:                           2                                         \nCovariance Type:            nonrobust                                         \n=================================================================================\n                    coef    std err          t      P&gt;|t|      [0.025      0.975]\n---------------------------------------------------------------------------------\nIntercept         0.4723      1.676      0.282      0.780      -2.956       3.900\nnp.log(sales)     0.3213      0.216      1.491      0.147      -0.120       0.762\nprofmarg          0.0500      0.046      1.093      0.283      -0.044       0.144\n==============================================================================\nOmnibus:                       15.836   Durbin-Watson:                   1.652\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               17.790\nSkew:                           1.430   Prob(JB):                     0.000137\nKurtosis:                       5.272   Cond. No.                         70.6\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\nusing WooldridgeDatasets, DataFrames, Statistics, GLM, Distributions\n\n# dados\nrdchem = DataFrame(wooldridge(\"rdchem\"));\n\nfit_julia = lm(@formula(rdintens ~ log(sales) + profmarg), rdchem);\ncoeficientes = DataFrame(Beta = coefnames(fit_julia), Valor = coef(fit_julia))\n\n3×2 DataFrame\n Row │ Beta         Valor\n     │ String       Float64\n─────┼────────────────────────\n   1 │ (Intercept)  0.472254\n   2 │ log(sales)   0.321348\n   3 │ profmarg     0.0500367\n\n\n\n\n\nA cada aumento de \\(1\\%\\) na variável sales é esperado um aumento de \\(\\frac{\\hat\\beta_1}{100} = 0.0032\\) unidades em rdintens (ver Tabela 1).\n\nTeste a hipóteses de que a intensidade de P&D não varia com sales contra a alternativa de que P&D aumenta com as vendas.\n\n\nRPythonJulia\n\n\n\nfit_r &lt;- lm(rdintens ~ sales, data=rdchem)\nsummary(fit_r)\n\n\nCall:\nlm(formula = rdintens ~ sales, data = rdchem)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-2.0531 -1.3234 -0.5280  0.8765  6.1146 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 3.064e+00  3.689e-01   8.308 2.84e-09 ***\nsales       5.318e-05  4.403e-05   1.208    0.237    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.86 on 30 degrees of freedom\nMultiple R-squared:  0.04638,   Adjusted R-squared:  0.01459 \nF-statistic: 1.459 on 1 and 30 DF,  p-value: 0.2365\n\n\n\n\n\n# ajustando modelo\nfit_py = smf.ols(formula='rdintens ~ sales', data=rdchem).fit()\nprint(fit_py.summary())\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:               rdintens   R-squared:                       0.046\nModel:                            OLS   Adj. R-squared:                  0.015\nMethod:                 Least Squares   F-statistic:                     1.459\nDate:                qui, 24 out 2024   Prob (F-statistic):              0.237\nTime:                        01:15:53   Log-Likelihood:                -64.231\nNo. Observations:                  32   AIC:                             132.5\nDf Residuals:                      30   BIC:                             135.4\nDf Model:                           1                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      3.0643      0.369      8.308      0.000       2.311       3.818\nsales       5.318e-05    4.4e-05      1.208      0.237   -3.67e-05       0.000\n==============================================================================\nOmnibus:                       15.661   Durbin-Watson:                   1.684\nProb(Omnibus):                  0.000   Jarque-Bera (JB):               17.224\nSkew:                           1.446   Prob(JB):                     0.000182\nKurtosis:                       5.135   Cond. No.                     9.40e+03\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The condition number is large, 9.4e+03. This might indicate that there are\nstrong multicollinearity or other numerical problems.\n\n\n\n\n\nusing HypothesisTests\n\nfit_julia = lm(@formula(rdintens ~ sales), rdchem);\nprintln(coeftable(fit_julia))\n\n─────────────────────────────────────────────────────────────────────────────\n                  Coef.  Std. Error     t  Pr(&gt;|t|)    Lower 95%    Upper 95%\n─────────────────────────────────────────────────────────────────────────────\n(Intercept)  3.06429     0.368855    8.31    &lt;1e-08   2.31098     3.81759\nsales        5.31799e-5  4.40251e-5  1.21    0.2365  -3.67314e-5  0.000143091\n─────────────────────────────────────────────────────────────────────────────\n\n\n\n\n\nNão, note que o , por ser um teste unilateral e o R executar um teste bilateral deve ser dividido por \\(2\\). Logo, \\(\\text{p-valor} = \\frac{0.237}{2} = 0.1185\\), portanto, ao nível de significância de \\(5\\%\\), não temos evidência para rejeitar \\(H_0\\) a favor de \\(H_1\\).\n\nInterprete o coeficiente de profmarg, ele é economicamente grande?\n\nInterpretação: a cada aumento de uma unidade em profmarg é esperado, em média e mantendo todos os outros preditores fixos, um aumento de \\(0.05\\) na variável resposta rdintens. Não, ele não é economicamente grande, note que não foi significativo.\n\nprofmarg tem um efeito estatisticamente significativo sobre rdintens?\n\nNão, o efeito não é estatísticamente significativo.\n\n\n\n\n\n\n\nQuestão 5\n\n\n\nUtilizando o dataset gpa1, ajuste um modelo que explique a nota média em um curso superior (colGPA) utilizando o número de faltas às aulas por semana (skipped), horas de estudo semanais (hsGPA) e a nota do ACT (equivalente ao vestitubular). Assumindo que as hipóteses do modelo linear clássico acontecem:\n\nEncontre um intervalo de confiança \\(95\\%\\) para \\(\\beta_{hsGPA}\\).\nTeste \\(H_0∶ \\beta_{hsGPA} = 0.4 \\;\\; V.S. \\;\\; H_1∶ \\beta_{hsGPA} \\neq 0.4\\).\nVocê pode rejeitar a hipóteses \\(H_0: \\beta_{hsGPA} = 1\\) contra a alternativa bilateral (\\(H_1: \\beta_{hsGPA} \\neq 1\\))?\nTeste a hipótese nula de que, uma vez tendo sido controlado as horas de estudo semanais, o efeito de skipped e ACT sobre colGPA são, conjuntamente, nulos.\n\n\n\n\nEncontre um intervalo de confiança \\(95\\%\\) para \\(\\beta_{hsGPA}\\).\n\n\nRPythonJulia\n\n\n\nmodel_r &lt;- lm(colGPA ~ skipped + hsGPA + ACT, data = gpa1)  # ajustando modelo\nconfint(model_r, level = 0.95)  # intervalo de confianca\n\n                   2.5 %      97.5 %\n(Intercept)  0.733929519  2.04517814\nskipped     -0.134523444 -0.03170283\nhsGPA        0.226581851  0.59705049\nACT         -0.006171074  0.03561154\n\n\n\n\n\n# lendo dados\ngpa1 = woo.dataWoo('gpa1')\ngpa1 = r['gpa1']  # leitura alternativa (direto do R)\n\n# ajustando modelo\nmodel_py = smf.ols(formula='colGPA ~ skipped + hsGPA + ACT', data=gpa1).fit()\nic = model_py.conf_int()\nic.columns = ['2.5%', '97.5%']\nprint(ic)\n\n               2.5%     97.5%\nIntercept  0.733930  2.045178\nskipped   -0.134523 -0.031703\nhsGPA      0.226582  0.597050\nACT       -0.006171  0.035612\n\n\n\n\n\ngpa1 = DataFrame(wooldridge(\"gpa1\"));\nmodel_julia = lm(@formula(colGPA ~ skipped + hsGPA + ACT), gpa1);\nresults = DataFrame(\n    Term = coefnames(model_julia),\n    LowerCI = confint(model_julia)[:, 1],\n    UpperCI = confint(model_julia)[:, 2]\n)\n\n4×3 DataFrame\n Row │ Term         LowerCI      UpperCI\n     │ String       Float64      Float64\n─────┼──────────────────────────────────────\n   1 │ (Intercept)   0.73393      2.04518\n   2 │ skipped      -0.134523    -0.0317028\n   3 │ hsGPA         0.226582     0.59705\n   4 │ ACT          -0.00617107   0.0356115\n\n\n\n\n\n\nTeste \\(H_0∶ \\beta_{hsGPA} = 0.4 \\;\\; V.S. \\;\\; H_1∶ \\beta_{hsGPA} \\neq 0.4\\).\n\n\nRPythonJulia\n\n\n\ncoef_hsGPA &lt;- coef(model_r)['hsGPA']\nep_hsGPA &lt;- summary(model_r)$coefficients['hsGPA', 'Std. Error']\n\nvalor_t &lt;- (coef_hsGPA - 0.4) / ep_hsGPA\n\ngl &lt;- model_r$df.residual\np_valor &lt;- 2*pt(-abs(valor_t), gl)  # P(|t| &gt; |t_obs|) = 2 P(t &lt; -|t_obs|) \n\ncat(paste0('Valor t:', round(valor_t, 4), '\\n', 'p-valor:', round(p_valor, 4)))\n\nValor t:0.1261\np-valor:0.8998\n\n\n\n\n\nimport scipy.stats as stats\n\ncoef_hsGPA = model_py.params['hsGPA']\nep_hsGPA = model_py.bse['hsGPA']\n\nvalor_t = (coef_hsGPA - 0.4) / ep_hsGPA\n\ngl = model_py.df_resid\np_valor = 2 * stats.t.cdf(-abs(valor_t), gl)  # P(|t| &gt; |t_obs|) = 2 P(t &lt; -|t_obs|) \n\nprint(f'Valor t: {valor_t:4f} \\np-valor: {p_valor:4f}')\n\nValor t: 0.126141 \np-valor: 0.899805\n\n\n\n\n\ncoef_hsGPA = coef(model_julia)[3];\nep_hsGPA = stderror(model_julia)[3];\n\nvalor_t = (coef_hsGPA - 0.4) / ep_hsGPA;\n\ngl = 137;\n\np_valor = 2 * cdf(TDist(gl), -abs(valor_t))\n\n0.8998051414602578\n\n\n\n\n\n\nVocê pode rejeitar a hipóteses \\(H_0: \\beta_{hsGPA} = 1\\) contra a alternativa bilateral (\\(H_1: \\beta_{hsGPA} \\neq 1\\))?\n\n\nRPythonJulia\n\n\n\nvalor_t &lt;- (coef_hsGPA - 1) / ep_hsGPA\n\ngl &lt;- model_r$df.residual\np_valor &lt;- 2*pt(-abs(valor_t), gl)  # P(|t| &gt; |t_obs|) = 2 P(t &lt; -|t_obs|) \n\ncat(paste0('Valor t:', round(valor_t, 4), '\\n', 'p-valor:', round(p_valor, 4)))\n\nValor t:-6.279\np-valor:0\n\n\n\n\n\ncoef_hsGPA = model_py.params['hsGPA']\nep_hsGPA = model_py.bse['hsGPA']\n\nvalor_t = (coef_hsGPA - 1) / ep_hsGPA\n\ngl = model_py.df_resid\np_valor = 2 * stats.t.cdf(-abs(valor_t), gl)  # P(|t| &gt; |t_obs|) = 2 P(t &lt; -|t_obs|) \n\nprint(f'Valor t: {valor_t:4f} \\np-valor: {p_valor:4f}')\n\nValor t: -6.279037 \np-valor: 0.000000\n\n\n\n\n\ncoef_hsGPA = coef(model_julia)[3];\nep_hsGPA = stderror(model_julia)[3];\n\nvalor_t = (coef_hsGPA - 1) / ep_hsGPA\n\n-6.279036501519401\n\n\ngl = 137;\n\np_valor = 2 * cdf(TDist(gl), -abs(valor_t))\n\n4.204251632295657e-9\n\n\n\n\n\n\nTeste a hipótese nula de que, uma vez tendo sido controlado as horas de estudo semanais, o efeito de skipped e ACT sobre colGPA são, conjuntamente, nulos.\n\nHipóteses: \\[\nH_0: \\beta_{skipped} = \\beta_{ACT} = 0 \\quad V.S. \\quad H_1: \\beta_{skipped} \\neq 0 \\text{ ou } \\beta_{ACT} \\neq 0\n\\]\nA estatística do teste é dada por:\n\\[\nF = \\frac{(SQR_r - SQR_i)/q}{SQR_i/(n-(k+1))} \\overset{H_0}{\\sim} F_{q, n-(k+1)}\n\\] onde \\(SQR_i\\) é a soma dos resíduos do modelo irrestrito (completo) e \\(SQR_r\\) é a soma dos resíduos do modelo reduzido, isto é, o modelo irrestrito sem as preditoras que se deseja testar.\n\nRPythonJulia\n\n\n\n# Ajustando modelo completo e reduzido\nmodelo_completo &lt;- lm(colGPA ~ skipped + hsGPA + ACT, data = gpa1)\nmodelo_reduzido &lt;- lm(colGPA ~ hsGPA, data = gpa1)\nanova(modelo_reduzido, modelo_completo)\n\nAnalysis of Variance Table\n\nModel 1: colGPA ~ hsGPA\nModel 2: colGPA ~ skipped + hsGPA + ACT\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    139 16.071                                \n2    137 14.873  2    1.1981 5.5179 0.004957 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nimport statsmodels.api as sm\n\n# ajustando modelo completo e reduzido\nmodelo_completo = smf.ols(formula='colGPA ~ skipped + hsGPA + ACT', data=gpa1).fit()\nmodelo_reduzido = smf.ols(formula='colGPA ~ hsGPA', data=gpa1).fit()\nsm.stats.anova_lm(modelo_reduzido, modelo_completo)\n\n   df_resid        ssr  df_diff   ss_diff         F    Pr(&gt;F)\n0     139.0  16.071039      0.0       NaN       NaN       NaN\n1     137.0  14.872966      2.0  1.198073  5.517931  0.004957\n\n\n\n\n\nmodelo_completo = lm(@formula(colGPA ~ skipped + hsGPA + ACT), gpa1);\nmodelo_reduzido = lm(@formula(colGPA ~ hsGPA), gpa1);\nftest(modelo_reduzido.model, modelo_completo.model)\n\nF-test: 2 models fitted on 141 observations\n────────────────────────────────────────────────────────────────\n     DOF  ΔDOF      SSR     ΔSSR      R²     ΔR²      F*   p(&gt;F)\n────────────────────────────────────────────────────────────────\n[1]    3        16.0710           0.1719                        \n[2]    5     2  14.8730  -1.1981  0.2336  0.0617  5.5179  0.0050\n────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\n\n\n\n\n\nQuestão 6\n\n\n\nUtilize o conjunto de dados wage2 e ajuste a regressão \\[\n\\log(wage) = \\beta_0 + \\beta_1 educ + \\beta_2 exper + \\beta_3 tenure + u\n\\] em que wage é o salario-hora em dolares, educ são os anos de educação formal, exper são os anos de experiência no mercado de trabalho e tenure são os anos de permanencia no emprego atual.\n\nTeste a hipótede de significância geral do modelo.\nTeste a hipótese de que um ano a mais de experiência no mercado de trabalho tem o mesmo efeito sobre \\(\\log(wage)\\) que mais um ano de permanencia no emprego atual.\nTeste a hipótese de que, controlado o número de anos de permanencia no emprego (tenure), educ e exper não tem efeito nenhum sobre \\(\\log(wage)\\).\n\n\n\n\nTeste a hipótese de significância geral do modelo.\n\n\nRPythonJulia\n\n\n\n# Ajustar o modelo de regressão\nfit_r &lt;- lm(log(wage) ~ educ + exper + tenure, data = wage2)\nsummary(fit_r)\n\n\nCall:\nlm(formula = log(wage) ~ educ + exper + tenure, data = wage2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.8282 -0.2401  0.0203  0.2569  1.3400 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 5.496696   0.110528  49.731  &lt; 2e-16 ***\neduc        0.074864   0.006512  11.495  &lt; 2e-16 ***\nexper       0.015328   0.003370   4.549 6.10e-06 ***\ntenure      0.013375   0.002587   5.170 2.87e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3877 on 931 degrees of freedom\nMultiple R-squared:  0.1551,    Adjusted R-squared:  0.1524 \nF-statistic: 56.97 on 3 and 931 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n\nwage2 = woo.dataWoo('wage2')\nwage2 = r['wage2']  # leitura alternativa (direto do R)\n\nfit_py = smf.ols('np.log(wage) ~ educ + exper + tenure', data=wage2).fit()\nfit_py.summary()\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\nnp.log(wage)\nR-squared:\n0.155\n\n\nModel:\nOLS\nAdj. R-squared:\n0.152\n\n\nMethod:\nLeast Squares\nF-statistic:\n56.97\n\n\nDate:\nqui, 24 out 2024\nProb (F-statistic):\n8.12e-34\n\n\nTime:\n01:16:08\nLog-Likelihood:\n-438.84\n\n\nNo. Observations:\n935\nAIC:\n885.7\n\n\nDf Residuals:\n931\nBIC:\n905.0\n\n\nDf Model:\n3\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n5.4967\n0.111\n49.731\n0.000\n5.280\n5.714\n\n\neduc\n0.0749\n0.007\n11.495\n0.000\n0.062\n0.088\n\n\nexper\n0.0153\n0.003\n4.549\n0.000\n0.009\n0.022\n\n\ntenure\n0.0134\n0.003\n5.170\n0.000\n0.008\n0.018\n\n\n\n\n\n\nOmnibus:\n20.917\nDurbin-Watson:\n1.769\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n30.558\n\n\nSkew:\n-0.214\nProb(JB):\n2.31e-07\n\n\nKurtosis:\n3.775\nCond. No.\n170.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\n\n\n\nwage2 = DataFrame(wooldridge(\"wage2\"));\nfit_julia = lm(@formula(log(wage) ~ educ + exper + tenure), wage2);\nftest(fit_julia.model)\n\nF-test against the null model:\nF-statistic: 56.97 on 935 observations and 3 degrees of freedom, p-value: &lt;1e-33\n\n\n\n\n\n\nTeste a hipótese de que um ano a mais de experiência no mercado de trabalho tem o mesmo efeito sobre \\(\\log(wage)\\) que mais um ano de permanencia no emprego atual.\n\n\nRPythonJulia\n\n\n\nrequire(car)\n\nfit_r &lt;- lm(log(wage) ~ educ + exper + tenure, data=wage2)\nlinearHypothesis(fit_r, 'exper - tenure = 0')\n\nLinear hypothesis test\n\nHypothesis:\nexper - tenure = 0\n\nModel 1: restricted model\nModel 2: log(wage) ~ educ + exper + tenure\n\n  Res.Df    RSS Df Sum of Sq      F Pr(&gt;F)\n1    932 139.99                           \n2    931 139.96  1  0.025502 0.1696 0.6805\n\n\n\n\n\nfit_py = smf.ols(formula='np.log(wage) ~ educ + exper + tenure', data=wage2).fit()\nrestricao = 'exper - tenure = 0'\nresultado = fit_py.wald_test(restricao)\nprint(resultado)\n\n&lt;F test: F=array([[0.16963746]]), p=0.6805290172334271, df_denom=931, df_num=1&gt;\n\n\n\n\n\n# function wald_test(;Sigma, b, Terms = nothing, L = nothing, H0 = nothing, df = nothing, print_result = true)\n#     if Terms == nothing && L == nothing\n#         error(\"One of the arguments Terms or L must be used.\")\n#     end\n#     if Terms != nothing && L != nothing\n#         error(\"Only one of the arguments Terms or L must be used.\")\n#     end\n#     if Terms == nothing\n#         w = size(L)[1]\n#         Terms = (1:length(b))[(sum(L, dims = 1) .&gt; 0)[1, :]]\n#     else\n#         w = length(Terms)\n#     end\n#     if H0 == nothing\n#         H0 = fill(0, w)\n#     end\n#     if w != length(H0)\n#         error(\"Vectors of tested coefficients and of null hypothesis have different lengths\\n\")\n#     end\n#     if L == nothing\n#         L = fill(0, w, length(b))\n#         for i in 1:w\n#             L[i, Terms[i]] = 1\n#         end\n#     end\n#     f = L * b\n#     V = Sigma\n#     mat = inv(L * V * L')\n#     statistic = (f - H0)' * mat * (f - H0)\n#     p = ccdf(Chisq(w), statistic)\n#     if df == nothing\n#         res = Dict(\"chi2\" =&gt; (chi2 = statistic, df = w, P = p))\n#     else\n#         fstat = statistic/size(L)[1]\n#         df1 = size(L)[1]\n#         df2 = df\n#         res = Dict(\"chi2\" =&gt; (chi2 = statistic, df = w, P = p),\n#             \"Ftest\" =&gt; (Fstat = fstat, df1 = df1, df2 = df2, P = ccdf(FDist(df1, df2), fstat)))\n#     end\n# \n#     if print_result == true\n#         println(\"Wald test:\\n\", \"----------\\n\\n\", \"Chi-squared test:\\n\",\n#             \"X2 = \", res[\"chi2\"].chi2, \", df = \", res[\"chi2\"].df, \", P(&gt; X2) = \", res[\"chi2\"].P)\n#         if df != nothing\n#             println(\"\\nF test:\\n\",\n#             \"W = \", res[\"Ftest\"].Fstat, \", df1 = \", res[\"Ftest\"].df1, \", df2 = \", res[\"Ftest\"].df2, \", P(&gt; W) = \", res[\"Ftest\"].P)\n#         end\n#     end\n# \n#     return (Sigma = Sigma, b = b, Terms = Terms, H0 = H0, L = L, result = res, df = df)\n# end\n# \n# # include(\"wald_test.jl\")\n# fit_julia_completo = lm(@formula(log(wage) ~ educ + exper + tenure), wage2);\n# fit_julia_reduzido = lm(@formula(log(wage) ~ educ + (exper + tenure), wage2);\n# wald_test(Sigma = vcov(fit_julia_completo), b = coef(fit_julia_reduzido), Terms = 2:3)\n# ftest(fit_julia_completo.model, fit_julia_reduzido.model)\n\n\n\n\n\nTeste a hipótese de que, controlado o número de anos de permanencia no emprego (tenure), educ e exper não tem efeito nenhum sobre \\(\\log(wage)\\).\n\n\nRPythonJulia\n\n\n\nmod_completo &lt;- lm(log(wage) ~ educ + exper + tenure, data=wage2)\nmod_reduzido &lt;- lm(log(wage) ~ tenure, data=wage2)\nanova(mod_reduzido, mod_completo)\n\nAnalysis of Variance Table\n\nModel 1: log(wage) ~ tenure\nModel 2: log(wage) ~ educ + exper + tenure\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    933 159.93                                 \n2    931 139.96  2    19.973 66.43 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nmod_completo = smf.ols(formula='np.log(wage) ~ educ + exper + tenure', data=wage2).fit()\nmod_reduzido = smf.ols(formula='np.log(wage) ~ tenure', data=wage2).fit()\nsm.stats.anova_lm(mod_reduzido, mod_completo)\n\n   df_resid         ssr  df_diff   ss_diff          F        Pr(&gt;F)\n0     933.0  159.934322      0.0       NaN        NaN           NaN\n1     931.0  139.960963      2.0  19.97336  66.429944  1.074911e-27\n\n\n\n\n\nfit_julia_completo = lm(@formula(log(wage) ~ educ + exper + tenure), wage2);\nfit_julia_reduzido = lm(@formula(log(wage) ~ tenure), wage2);\nftest(fit_julia_completo.model, fit_julia_reduzido.model)\n\nF-test: 2 models fitted on 935 observations\n───────────────────────────────────────────────────────────────────\n     DOF  ΔDOF       SSR     ΔSSR      R²      ΔR²       F*   p(&gt;F)\n───────────────────────────────────────────────────────────────────\n[1]    5        139.9610           0.1551                          \n[2]    3    -2  159.9343  19.9734  0.0345  -0.1206  66.4299  &lt;1e-26\n───────────────────────────────────────────────────────────────────\n\n\n\n\n\n\n\n\n\n\n\n\nQuestão 7\n\n\n\nUtilize o conjunto de dados htv e ajuste a regressão \\[\neduc = \\beta_0 + \\beta_1 motheduc + \\beta_2 fatheduc + \\beta_3 abil + \\beta_4 abil^2 + u\n\\]\n\nTeste a hipóteses de que a influencia que motheduc e fatheduc exercem sobre educ é a mesma.\nTeste a hipótese de que educ está linearmente relacionado com abil contra a alternativa que diz que a relação é quadrática.\nUm colega de trabalho diz que o modelo \\(educ = \\beta_0 + \\beta_1 abil + \\beta_2 abil^2 + u\\) é suficiente, e que tanto motheduc e fatheduc não são importantes para o modelos. Faça um teste de hipóteses para rejeitar ou não rejeitar a hipótese do seu colega.\n\n\n\n\nTeste a hipóteses de que a influencia que motheduc e fatheduc exercem sobre educ é a mesma.\n\n\nRPythonJulia\n\n\n\n# require(car)\n\nfit_r &lt;- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), data=htv)\nlinearHypothesis(fit_r, 'motheduc - fatheduc = 0')\n\nLinear hypothesis test\n\nHypothesis:\nmotheduc - fatheduc = 0\n\nModel 1: restricted model\nModel 2: educ ~ motheduc + fatheduc + abil + I(abil^2)\n\n  Res.Df    RSS Df Sum of Sq      F  Pr(&gt;F)  \n1   1226 3796.8                              \n2   1225 3785.2  1    11.578 3.7468 0.05314 .\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nhtv = woo.dataWoo('htv')\nhtv = r['htv']  # leitura alternativa (direto do R)\n\nfit_py = smf.ols('educ ~ motheduc + fatheduc + abil + I(abil**2)', data=htv).fit()\nrestricao = 'motheduc - fatheduc = 0'\nresultado = fit_py.wald_test(restricao)\n\nprint(resultado)\n\n&lt;F test: F=array([[3.74676464]]), p=0.05313970412965828, df_denom=1.22e+03, df_num=1&gt;\n\n\n\n\n\n\n\n\n\nTeste a hipótese de que educ está linearmente relacionado com abil contra a alternativa que diz que a relação é quadrática.\n\n\nRPythonJulia\n\n\n\nfit_completo &lt;- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), data=htv)\nfit_reduzido &lt;- lm(educ ~ motheduc + fatheduc + abil, data=htv)\nanova(fit_completo, fit_reduzido)\n\nAnalysis of Variance Table\n\nModel 1: educ ~ motheduc + fatheduc + abil + I(abil^2)\nModel 2: educ ~ motheduc + fatheduc + abil\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1   1225 3785.2                                 \n2   1226 3900.0 -1   -114.73 37.13 1.478e-09 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nfit_completo = smf.ols(formula='educ ~ motheduc + fatheduc + abil + I(abil**2)',\n                       data=htv).fit()\nfit_reduzido = smf.ols(formula='educ ~ motheduc + fatheduc + abil', data=htv).fit()\nsm.stats.anova_lm(fit_reduzido, fit_completo)\n\n   df_resid          ssr  df_diff     ss_diff          F        Pr(&gt;F)\n0    1226.0  3899.972622      0.0         NaN        NaN           NaN\n1    1225.0  3785.242616      1.0  114.730006  37.129524  1.478065e-09\n\n\n\n\n\nhtv = DataFrame(wooldridge(\"htv\"));\nfit_julia_completo = lm(@formula(educ ~ motheduc + fatheduc + abil + abil^2), htv);\nfit_julia_reduzido = lm(@formula(educ ~ motheduc + fatheduc + abil), htv);\nftest(fit_julia_completo.model, fit_julia_reduzido.model)\n\nF-test: 2 models fitted on 1230 observations\n─────────────────────────────────────────────────────────────────────\n     DOF  ΔDOF        SSR      ΔSSR      R²      ΔR²       F*   p(&gt;F)\n─────────────────────────────────────────────────────────────────────\n[1]    6        3785.2426            0.4444                          \n[2]    5    -1  3899.9726  114.7300  0.4275  -0.0168  37.1295  &lt;1e-08\n─────────────────────────────────────────────────────────────────────\n\n\n\n\n\n\nUm colega de trabalho diz que o modelo \\(educ = \\beta_0 + \\beta_1 abil + \\beta_2 abil^2 +𝑢\\) é suficiente, e que tanto motheduc e fatheduc não são importantes para o modelos. Faça um teste de hipóteses para rejeitar ou não rejeitar a hipótese do seu colega.\n\n\nRPythonJulia\n\n\n\nfit_completo &lt;- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), data=htv)\nfit_reduzido &lt;- lm(educ ~ abil + I(abil^2), data=htv)\nanova(fit_completo, fit_reduzido)\n\nAnalysis of Variance Table\n\nModel 1: educ ~ motheduc + fatheduc + abil + I(abil^2)\nModel 2: educ ~ abil + I(abil^2)\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1   1225 3785.2                                  \n2   1227 4287.9 -2   -502.63 81.333 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\n\n\n\nfit_completo = smf.ols(formula='educ ~ motheduc + fatheduc + abil + I(abil**2)',\n                       data=htv).fit()\nfit_reduzido = smf.ols(formula='educ ~ abil + I(abil**2)', data=htv).fit()\nsm.stats.anova_lm(fit_reduzido, fit_completo)\n\n   df_resid          ssr  df_diff     ss_diff          F        Pr(&gt;F)\n0    1227.0  4287.877365      0.0         NaN        NaN           NaN\n1    1225.0  3785.242616      2.0  502.634749  81.332642  6.822776e-34\n\n\n\n\n\nfit_julia_completo = lm(@formula(educ ~ motheduc + fatheduc + abil + abil^2), htv);\nfit_julia_reduzido = lm(@formula(educ ~ abil + abil^2), htv);\nftest(fit_julia_completo.model, fit_julia_reduzido.model)\n\nF-test: 2 models fitted on 1230 observations\n─────────────────────────────────────────────────────────────────────\n     DOF  ΔDOF        SSR      ΔSSR      R²      ΔR²       F*   p(&gt;F)\n─────────────────────────────────────────────────────────────────────\n[1]    6        3785.2426            0.4444                          \n[2]    4    -2  4287.8774  502.6347  0.3706  -0.0738  81.3326  &lt;1e-33\n─────────────────────────────────────────────────────────────────────"
  },
  {
    "objectID": "scripts/ic/politica_de_limiar.html",
    "href": "scripts/ic/politica_de_limiar.html",
    "title": "Simulação: Política de Limiar",
    "section": "",
    "text": "\\[\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\posto}{posto}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\newcommand{\\ind}{\\mathcal{I}}      % Função indicadora\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\p}{\\mathbb{P}}\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\newcommand{\\bm}[1]{\\mathbf{#1}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\NN}{\\mathbb{N}}\n\\newcommand{\\LL}{\\mathcal{L}}       % Verosimilhança\n\\newcommand{\\norm}{\\mathcal{N}}\n\\newcommand{\\spc}{\\hspace{0.8cm}}   % Espaçamento\n\\]"
  },
  {
    "objectID": "scripts/ic/politica_de_limiar.html#universal-threshold",
    "href": "scripts/ic/politica_de_limiar.html#universal-threshold",
    "title": "Simulação: Política de Limiar",
    "section": "Universal Threshold",
    "text": "Universal Threshold\nA proposta deste método para o cálculo do limiar é dada por \\[\\lambda_u = \\sigma\\sqrt{2 \\ln(n)}\\] onde \\(\\sigma\\) é estimado pelo MAD (median absolute deviation) dos coeficientes empíricos presentes na escala mais fina. Esse limiar leva a estimadores que subestimam \\(f\\), já que apresenta uma tendência de aplicar o encolhimento em muitos coeficientes, principalmente em níveis de resolução mais finos."
  },
  {
    "objectID": "scripts/ic/politica_de_limiar.html#sure-threshold",
    "href": "scripts/ic/politica_de_limiar.html#sure-threshold",
    "title": "Simulação: Política de Limiar",
    "section": "SURE Threshold",
    "text": "SURE Threshold\nEste método utiliza como critério para a escolha do limiar \\(\\lambda\\) o valor que minimiza um estimador não viesado do risco de Stein (Stein unbiased risk estimator) \\(SURE(\\lambda; \\mathbf{y})\\), dado por \\[SURE(\\lambda; \\mathbf{y}) = n - 2\\#\\{i:|y_i| \\leq \\lambda\\} + \\sum_{i}(\\min\\{|y_i|, \\lambda\\})^2\\] Além disso, vale notar que este método não funciona bem quando os coeficientes do sinal verdadeiro são muito esparsos."
  },
  {
    "objectID": "scripts/ic/politica_de_limiar.html#validação-cruzada-cv",
    "href": "scripts/ic/politica_de_limiar.html#validação-cruzada-cv",
    "title": "Simulação: Política de Limiar",
    "section": "Validação Cruzada (CV)",
    "text": "Validação Cruzada (CV)\nO método de validação cruzada consiste em utilizar uma parcela dos dados para ajustar o modelo, enquanto que a parte restante é utilizada para avaliar sua adequação com o objetivo de estimar \\(f\\). No caso das ondaletas a DWT precisa ser aplicada em um vetor diádico, portanto, faz-se necessária a utilização de um vetor com \\(2^J\\) elementos. Logo, é utilizado metade das observações para ajuste do modelo e a outra metade na sua validação. O algorítimo do processo é dado por:\n\nRemover as observações \\(y_i\\) com subíndice ímpar do conjunto de dados, logo, tem-se \\(2^{J-1}\\) observações, as quais devem ser reindexadas para \\(k=1, \\dots, 2^{J-1}\\);\nEstimar a função \\(f\\) por meio de \\(\\hat f^{par}_\\lambda\\), utilizando um limiar particular \\(\\lambda\\) advindo dos dados reindexados;\nUtilizar os dados retirados com subíndice ímpar para formar uma versão interpolada dos dados ímpares, dada por \\[\\bar{y}_k^{impar} = \\begin{cases}\n   (y_{2k - 1} + y_{2k + 1})/2 &,\\; k=1,\\dots, \\frac{n}{2}-1\\\\\n   (y_1 + y_{n-1})/2 &,\\; k=\\frac{n}{2}\n    \\end{cases}\\]\nDe forma similar, deve-se obter \\(\\hat f_\\lambda^{impar}\\) e \\(\\bar y_k^{par}\\) e calcular a estimativa do erro, dada por \\[\\hat M(\\lambda) = \\sum_k \\left[ (\\hat f_\\lambda^{par}(y_k) - \\bar y_k^{impar})^2 + (\\hat f_\\lambda^{impar}(y_k) - \\bar y_k^{par})^2 \\right]\n     \\tag{1}\\]\nRepetir o processo para uma faixa de valores de \\(\\lambda\\) e escolher o valor que minimize a Eq. 1."
  },
  {
    "objectID": "scripts/ic/politica_de_limiar.html#false-discovery-rate-fdr",
    "href": "scripts/ic/politica_de_limiar.html#false-discovery-rate-fdr",
    "title": "Simulação: Política de Limiar",
    "section": "False Discovery Rate (FDR)",
    "text": "False Discovery Rate (FDR)\nNeste método a escolha do limiar consiste em realizar testes de hipóteses de forma recursiva para cada nível de resolução e para cada coeficiente de ondaletas, testando se o coeficiente é zero (ruído) ou diferente de zero (sinal). O procedimento se dá por:\n\nCalcular o p-valor \\(p_{jk}\\) para cada coeficiente \\(d_{jk}\\), testando \\(H_0:d_{jk} = 0\\) contra \\(H_1:d_{jk} \\neq 0\\), isto é, \\[p_{jk} = 2 \\left( 1 - \\Phi\\left( \\frac{|d^*_{jk}|}{\\sigma} \\right) \\right)\\]\nOrdenar os p-valores obtidos e reindexá-los com base no seu valor, logo, \\(p_{(1)} \\leq \\dots \\leq p_{(n)}\\);\nSeja \\(q\\) o valor que representa o nível determinado para a taxa de coeficientes atribuídos erroneamente como não nulos e \\(i_0\\) o maior \\(i\\) tal que \\(p_{(i)} \\leq \\left(\\frac{i}{m} \\right)q\\). Para este \\(i_0\\) calcule o valor do limiar, dado por \\[\\lambda_{i_0} = \\sigma \\Phi^{-1} \\left( 1 - \\frac{p_{i_0}}{2} \\right)\\]"
  },
  {
    "objectID": "scripts/ic/politica_de_limiar.html#proposta-bayesiana-sob-priori-logística-sousa-2020",
    "href": "scripts/ic/politica_de_limiar.html#proposta-bayesiana-sob-priori-logística-sousa-2020",
    "title": "Simulação: Política de Limiar",
    "section": "Proposta Bayesiana sob Priori Logística (Sousa, 2020)",
    "text": "Proposta Bayesiana sob Priori Logística (Sousa, 2020)\nPriori atribuída: \\[\\pi(d) = \\alpha \\delta_0 + (1 - \\alpha) g(d; \\tau)\\] onde \\(\\alpha \\in (0,1)\\), \\(\\delta_0\\) é o delta de Dirac com massa em \\(0\\) e \\(g(d; \\tau)\\) é a função densidade de probabilidade logística, dada por: \\[\ng(d; \\tau) = \\frac{\\exp\\left\\{-\\frac{d}{\\tau}\\right\\}}{\\tau \\left( 1 + \\exp\\left\\{-\\frac{d}{\\tau}\\right\\} \\right)^2} \\;\\ind_\\RR(d) \\;, \\spc \\tau &gt; 0\n\\]\nRegra de encolhimento: \\[\\delta(d) = \\frac{(1-\\alpha) \\int_\\RR (\\sigma u + d) g(\\sigma u + d; \\tau) \\phi(u) \\,du}{\\frac{\\alpha}{\\sigma} \\phi\\left(\\frac{d}{\\sigma} \\right) + (1-\\alpha) \\int_\\RR g(\\sigma u + d; \\tau) \\phi(u) \\, du}\\]\nDemonstração. Considere nesta demonstração, para melhor denotar os termos, que \\(d=\\theta\\) e a substituição \\(u = \\frac{\\theta - d^*}{\\sigma}\\). Então, \\[\\begin{align*}\n    \\delta(\\theta) &= \\E(\\theta \\mid d^*) = \\int_\\RR \\theta \\,\\pi(\\theta\\mid d^*) \\,d\\theta\\\\\n    &=\\frac{\\int_\\RR \\theta \\,\\pi(\\theta) \\LL(\\theta\\mid d^*) \\,d\\theta}{\\int_\\RR \\pi(\\theta) \\LL(\\theta\\mid d^*) \\,d\\theta}\\\\\n    &=\\frac{\\int_\\RR \\theta [\\alpha \\delta_0 + (1 - \\alpha) g(\\theta; \\tau)] \\LL(\\theta\\mid d^*) \\,d\\theta}{\\int_\\RR [\\alpha \\delta_0 + (1 - \\alpha) g(\\theta; \\tau)] \\LL(\\theta\\mid d^*) \\,d\\theta}\\\\\n    &=\\frac{\\alpha\\textstyle\\int_\\RR\\theta \\, \\delta_0(\\theta) \\LL(\\theta\\mid d^*) \\,d\\theta + (1-\\alpha) \\int_\\RR \\theta \\, g(\\theta;\\tau) \\LL(\\theta\\mid d^*) \\, d\\theta}{\\alpha \\textstyle\\int_\\RR \\delta_0(\\theta) \\LL(\\theta\\mid d^*) \\, d\\theta + (1-\\alpha) \\int_\\RR g(\\theta; \\tau) \\LL(\\theta\\mid d^*) \\, d\\theta}\\\\\n    &=\\frac{(1-\\alpha) \\int_\\RR \\theta\\, g(\\theta; \\tau) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{ -\\frac{1}{2} \\left( \\frac{d^* - \\theta}{\\sigma} \\right)^2 \\right\\} \\frac{1}{\\sigma} \\, d\\theta}\n    {\\frac{\\alpha}{\\sigma\\sqrt{2\\pi}}\\exp\\left\\{ -\\frac{(d^*-0)^2}{2\\sigma^2} \\right\\} + (1-\\alpha) \\int_\\RR g(\\theta; \\tau) \\frac{1}{\\sqrt{2\\pi}} \\exp\\left\\{ -\\frac{1}{2}\\left(\\frac{d^* - \\theta}{\\sigma}\\right)^2 \\right\\} \\frac{1}{\\sigma} \\,d\\theta}\\\\\n    \\delta(\\theta) &= \\frac{(1-\\alpha) \\int_\\RR (\\sigma u + d^*) g(\\sigma u + d^*; \\tau) \\phi(u) \\,du}{\\frac{\\alpha}{\\sigma} \\phi(\\frac{d^*}{\\sigma}) + (1 - \\alpha) \\int_\\RR g(\\sigma u + d^*; \\tau) \\phi(u) \\,du}\n\\end{align*}\\]"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html",
    "title": "Modelo SUR",
    "section": "",
    "text": "\\[\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\posto}{posto}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\V}{\\mathbb{V}}\n\\DeclareMathOperator{\\E}{\\mathbb{E}}\n\\DeclareMathOperator{\\p}{\\mathbb{P}}\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\newcommand{\\bm}[1]{\\mathbf{#1}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\newcommand{\\NN}{\\mathbb{N}}\n\\newcommand{\\bu}{\\bullet}\n\\newcommand{\\norm}{\\mathcal{N}}\n\\newcommand{\\spc}{\\hspace{0.8cm}}   % Espaçamento\n\\]\n\n\n\n\nO que é o Modelo SUR?\nPor que utilizar o Modelo SUR?\nComo estimar o Modelo SUR?\nModelo SUR ou MQO? Quais as vantagens e desvantagens?\n\n\n\n\nOs modelos de regressões lineares aparentemente não correlacionados ou modelo SUR (Seemingly Unrelated Regression) é um modelo multivariado, isto é, busca explicar múltiplas variáveis dependentes. Além disso, pode ser definido através de \\(g\\) modelos lineares indexados por \\(i\\), dados por:\n\\[\\begin{equation}\\tag{1}\\label{eq: modelo sur}\n\\bs y_i = \\bs X_i \\bs\\beta_i + \\bs u_i \\;, \\spc i=1,\\dots,g\n\\end{equation}\\]\nonde\n\n\\(\\bs y_i\\) denota o vetor de tamanho \\(n\\) da \\(i\\)-ésima variável dependente;\n\\(\\bs X_i\\) denota a matriz de dimensão \\(n \\times k_i\\) de regressores da \\(i\\)-ésima equação;\n\\(\\bs\\beta_i\\) denota o vetor \\(k_i\\)-dimensional de parâmetros;\n\\(\\bs u_i\\) denota o vetor de erros de tamanho \\(n\\).\n\n\n\n\nO modelo SUR pode ser escrito da seguinte forma:\n\\[\\bs y_\\bu = \\bs X_\\bu \\bs\\beta_\\bu + \\bs u_\\bu\\]\n\\[\n  \\begin{bmatrix}\n      \\bs y_1 \\\\ \\bs y_2 \\\\ \\vdots \\\\ \\bs y_g\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n      \\bs X_1 & \\bs0    & \\dots  & \\bs0\\\\\n      \\bs0    & \\bs X_2 & \\dots  & \\bs0\\\\\n      \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n      \\bs0    & \\bs0    & \\dots  & \\bs X_g\\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\bs\\beta_1 \\\\ \\bs\\beta_2 \\\\ \\vdots \\\\ \\bs\\beta_g\n  \\end{bmatrix} +\n  \\begin{bmatrix}\n      \\bs u_1 \\\\ \\bs u_2 \\\\ \\vdots \\\\ \\bs u_g\n  \\end{bmatrix}\n\\]\nonde,\n\n\\(\\bs y_\\bu\\) é um vetor de tamanho \\(ng\\) com os vetores \\(\\bs y_1, \\dots, \\bs y_g\\) empilhados;\n\\(\\bs X_\\bu\\) é uma matriz diagonal com blocos \\(\\bs X_1, \\dots, \\bs X_g\\);\n\\(\\bs\\beta_\\bu\\) contém os vetores \\(\\bs\\beta_1, \\dots, \\bs\\beta_g\\) empilhados;\n\\(\\bs u_\\bu\\) é um vetor de tamanho \\(ng\\) com os vetores \\(\\bs u_1, \\dots, \\bs u_g\\) empilhados.\n\n\n\n\n\n\\(\\E(u_i \\mid \\bs X_i) = 0\\), \\(\\forall i\\), isto é, fracamente exógenos (então, por que não utilizar MQO? 🤔);\n\\(\\E(\\bs u_i \\bs u_i') = \\sigma_{ii} I_n\\) e \\(\\E(u_{ti}u_{tj}) = \\sigma_{ij}\\), ou seja, a variância do erro é constante em cada equação, mas pode variar entre equações;\n\\(\\E(u_{ti}u_{sj}) = 0\\), \\(t \\neq s\\) e \\(i \\neq j\\).\n\n. . .\nonde \\(\\sigma_{ij}\\) é o elemento \\((ij)\\) da matriz \\(\\Sigma_{g \\times g} &gt; 0\\), denominada de matriz de covariância contemporânea.\n. . .\n Portanto, fica definido o modelo SUR por meio das \\(g\\) equações em \\(\\eqref{eq: modelo sur}\\) e das suposições apresentadas.\n\n\n\n\nCorrelação contemporânea?\n\n\n\n\n. . .\nA correlação contemporânea é quando existe correlação entre os erros dos diferentes modelos para um mesmo indivíduo. Seu surgimento é devido à omissão de variáveis, já que as informações presentes nas preditoras que explicam a variável resposta (mas não entram no modelo) passam a fazer parte do erro.\n\n\n\n\nVantagem: Com a adaptação feita, o modelo passa a considerar a correlação presente no erro, aumentando sua precisão.\nDesvantagem: Quando o número de indivíduos é muito grande, o número de parâmetros a ser estimado também será, podendo levar à estimativas pouco fidedignas do modelo."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#motivação",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#motivação",
    "title": "Modelo SUR",
    "section": "Motivação",
    "text": "Motivação\n\nO que é o Modelo SUR?\nPor que utilizar o Modelo SUR?\nComo estimar o Modelo SUR?\nModelo SUR ou MQO? Quais as vantagens e desvantagens?"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-sur-1",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-sur-1",
    "title": "Modelo SUR",
    "section": "Modelo SUR",
    "text": "Modelo SUR\nOs modelos de regressões lineares aparentemente não correlacionados ou modelo SUR (Seemingly Unrelated Regression) é um modelo multivariado, isto é, busca explicar múltiplas variáveis dependentes. Além disso, pode ser definido através de \\(g\\) modelos lineares indexados por \\(i\\), dados por:\n\\[\\begin{equation}\\tag{1}\\label{eq: modelo sur}\n\\bs y_i = \\bs X_i \\bs\\beta_i + \\bs u_i \\;, \\spc i=1,\\dots,g\n\\end{equation}\\]\nonde\n\n\\(\\bs y_i\\) denota o vetor de tamanho \\(n\\) da \\(i\\)-ésima variável dependente;\n\\(\\bs X_i\\) denota a matriz de dimensão \\(n \\times k_i\\) de regressores da \\(i\\)-ésima equação;\n\\(\\bs\\beta_i\\) denota o vetor \\(k_i\\)-dimensional de parâmetros;\n\\(\\bs u_i\\) denota o vetor de erros de tamanho \\(n\\)."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-sur-2",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-sur-2",
    "title": "Modelo SUR",
    "section": "Modelo SUR",
    "text": "Modelo SUR\nO modelo SUR pode ser escrito da seguinte forma:\n\\[\\bs y_\\bu = \\bs X_\\bu \\bs\\beta_\\bu + \\bs u_\\bu\\]\n\\[\n  \\begin{bmatrix}\n      \\bs y_1 \\\\ \\bs y_2 \\\\ \\vdots \\\\ \\bs y_g\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n      \\bs X_1 & \\bs0    & \\dots  & \\bs0\\\\\n      \\bs0    & \\bs X_2 & \\dots  & \\bs0\\\\\n      \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n      \\bs0    & \\bs0    & \\dots  & \\bs X_g\\\\\n  \\end{bmatrix}\n  \\begin{bmatrix}\n      \\bs\\beta_1 \\\\ \\bs\\beta_2 \\\\ \\vdots \\\\ \\bs\\beta_g\n  \\end{bmatrix} +\n  \\begin{bmatrix}\n      \\bs u_1 \\\\ \\bs u_2 \\\\ \\vdots \\\\ \\bs u_g\n  \\end{bmatrix}\n\\]\nonde,\n\n\\(\\bs y_\\bu\\) é um vetor de tamanho \\(ng\\) com os vetores \\(\\bs y_1, \\dots, \\bs y_g\\) empilhados;\n\\(\\bs X_\\bu\\) é uma matriz diagonal com blocos \\(\\bs X_1, \\dots, \\bs X_g\\);\n\\(\\bs\\beta_\\bu\\) contém os vetores \\(\\bs\\beta_1, \\dots, \\bs\\beta_g\\) empilhados;\n\\(\\bs u_\\bu\\) é um vetor de tamanho \\(ng\\) com os vetores \\(\\bs u_1, \\dots, \\bs u_g\\) empilhados."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#suposições",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#suposições",
    "title": "Modelo SUR",
    "section": "Suposições",
    "text": "Suposições\n\n\\(\\E(u_i \\mid \\bs X_i) = 0\\), \\(\\forall i\\), isto é, fracamente exógenos (então, por que não utilizar MQO? 🤔);\n\\(\\E(\\bs u_i \\bs u_i') = \\sigma_{ii} I_n\\) e \\(\\E(u_{ti}u_{tj}) = \\sigma_{ij}\\), ou seja, a variância do erro é constante em cada equação, mas pode variar entre equações;\n\\(\\E(u_{ti}u_{sj}) = 0\\), \\(t \\neq s\\) e \\(i \\neq j\\).\n\n\nonde \\(\\sigma_{ij}\\) é o elemento \\((ij)\\) da matriz \\(\\Sigma_{g \\times g} &gt; 0\\), denominada de matriz de covariância contemporânea.\n\n\n Portanto, fica definido o modelo SUR por meio das \\(g\\) equações em \\(\\eqref{eq: modelo sur}\\) e das suposições apresentadas."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#suposições-1",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#suposições-1",
    "title": "Modelo SUR",
    "section": "Suposições",
    "text": "Suposições\n\nCorrelação contemporânea?\n\n\n\n\n\nA correlação contemporânea é quando existe correlação entre os erros dos diferentes modelos para um mesmo indivíduo. Seu surgimento é devido à omissão de variáveis, já que as informações presentes nas preditoras que explicam a variável resposta (mas não entram no modelo) passam a fazer parte do erro."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#vantagens-e-desvantagens",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#vantagens-e-desvantagens",
    "title": "Modelo SUR",
    "section": "Vantagens e Desvantagens",
    "text": "Vantagens e Desvantagens\n\nVantagem: Com a adaptação feita, o modelo passa a considerar a correlação presente no erro, aumentando sua precisão.\nDesvantagem: Quando o número de indivíduos é muito grande, o número de parâmetros a ser estimado também será, podendo levar à estimativas pouco fidedignas do modelo."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#produto-de-kronecker",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#produto-de-kronecker",
    "title": "Modelo SUR",
    "section": "Produto de Kronecker",
    "text": "Produto de Kronecker\nSeja \\(A\\) uma matriz \\(p \\times q\\) e \\(B\\) uma matriz \\(r \\times s\\). Então, o produto de Kronecker, denotado por \\(\\otimes\\), entre \\(A\\) e \\(B\\) é dado por: \\[\nA \\otimes B = \\begin{bmatrix}\n  a_{11} B & \\dots  & a_{1q}B\\\\\n  \\vdots   & \\ddots & \\vdots\\\\\n  a_{p1} B & \\dots  & a_{pq}B\\\\\n\\end{bmatrix}\n\\]\nPropriedades:\n\n\\((A \\otimes B)' = A' \\otimes B'\\)\n\\((A \\otimes B)(C \\otimes D) = (AC) \\otimes (BD)\\)\n\\((A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}\\)"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#estimação-1",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#estimação-1",
    "title": "Modelo SUR",
    "section": "Estimação",
    "text": "Estimação\nO estimador de MQO para o modelo SUR é dado por: \\[\\bs{\\hat\\beta}_{\\bu}^{MQO} = (\\bs X_\\bu'\\bs X_\\bu)^{-1}\\bs X_\\bu' \\bs y_\\bu\\]\n\nAlém disso,\n\\[\n\\E(\\bs u_\\bu \\bs u_\\bu') =\n\\begin{bmatrix}\n  \\E(\\bs u_1 \\bs u_1') & \\cdots  & \\E(\\bs u_1 \\bs u_g')\\\\\n  \\vdots               & \\ddots & \\vdots\\\\\n  \\E(\\bs u_g \\bs u_1')         & \\cdots  & \\E(\\bs u_g \\bs u_g')\n\\end{bmatrix} =\n\\begin{bmatrix}\n  \\sigma_{11} I_n & \\cdots & \\sigma_{1g} I_n\\\\\n  \\vdots          & \\ddots & \\vdots\\\\\n  \\sigma_{g1} I_n & \\cdots & \\sigma_{gg} I_n\n\\end{bmatrix} = \\bs\\Sigma_\\bu\n\\] mas \\(\\bs\\Sigma_\\bu = \\Sigma \\otimes I_n \\neq \\sigma I\\).\n\n\n\nPortanto, o estimador por MQO não é BLUE 😔."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#mqg",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#mqg",
    "title": "Modelo SUR",
    "section": "MQG",
    "text": "MQG\nPara o caso de \\(\\Sigma\\) conhecida, pode-se utilizar MQG, então\n\n\\[\\bs{\\hat\\beta}_\\bu^{MQG} = (\\bs X_\\bu' (\\Sigma^{-1} \\otimes I_n) \\bs X_\\bu)^{-1} \\bs X_\\bu' (\\Sigma^{-1} \\otimes I_n) \\bs y_\\bu\\]\n\\[\\V(\\bs{\\hat\\beta}_\\bu^{MQG} \\mid \\bs X) = (\\bs X_\\bu' (\\Sigma^{-1} \\otimes I_n) \\bs X_\\bu)^{-1} =\n\\begin{bmatrix}\n    \\sigma_{11} \\bs X_1' \\bs X_1 & \\cdots & \\sigma_{ig} \\bs X_1' \\bs X_g\\\\\n    \\vdots & \\ddots & \\vdots\\\\\n    \\sigma_{g1} \\bs X_g' \\bs X_1 & \\cdots & \\sigma_{gg} \\bs X_g' \\bs X_g\n\\end{bmatrix}^{-1}\\]"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#por-que-não-mqo",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#por-que-não-mqo",
    "title": "Modelo SUR",
    "section": "Por que não MQO?",
    "text": "Por que não MQO?\n\nSe \\(\\sigma_{ij} = 0\\), \\(\\forall i \\neq j\\), então não há ganhos em aplicar outro método que não seja MQO;\nSe \\(\\bs X_i = \\bs X_j\\), \\(\\forall i,j\\), então MQO e MQG são iguais;\nQuanto maior a correlação entre \\(u_{ti}\\) e \\(u_{tj}\\), então maior a eficiência de MQG sobre MQO;\nQuanto menor for a correlação entre \\(\\bs X_i\\) e \\(\\bs X_j\\), \\(i \\neq j\\) maior serão os ganhos em utilizar MQG do que MQO."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#mqgf",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#mqgf",
    "title": "Modelo SUR",
    "section": "MQGF",
    "text": "MQGF\nContudo, no geral, \\(\\Sigma\\) não é conhecida. Então, por MQGF:\n\n\\[\\hat\\Sigma = \\frac{1}{n} \\bm{\\hat U}'\\bm{\\hat U}\\] onde \\(\\bm{\\hat U} = \\begin{bmatrix} \\bs u_1 & \\cdots & \\bs u_g \\end{bmatrix}\\), isto é, uma matriz \\(n \\times g\\) cujas colunas são os vetores de resíduo \\(\\bs{\\hat u}_i\\) da \\(i\\)-ésima equação, obtidos por MQO. Além disso, utilizar \\(n\\) produz um estimador viesado, então, se \\(k_i = k_j = k\\), \\(\\forall i\\neq j\\), pode ser utilizado \\(n-k\\) para obter um estimador não viesado para os elementos da diagonal de \\(\\hat\\Sigma\\).\n\n\n\\[\\bs{\\hat\\beta}_\\bu^{MQGF} = (\\bs X_\\bu' (\\hat\\Sigma^{-1} \\otimes I_n) \\bs X_\\bu)^{-1} \\bs X_\\bu' (\\hat\\Sigma^{-1} \\otimes I_n)\\bs y_\\bu\\]\n\\[\\V(\\bs{\\hat\\beta}^{MQGF} \\mid \\bs X) = (\\bs X_\\bu' (\\hat\\Sigma^{-1} \\otimes I_n) \\bs X_\\bu)^{-1}\\]\n\n\nAdemais, substituir \\(\\Sigma\\) por \\(\\hat\\Sigma\\) afeta as propriedades de MQG, principalmente quando \\(n\\) é pequeno e \\(g\\) é grande."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#simulação",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#simulação",
    "title": "Modelo SUR",
    "section": "Simulação",
    "text": "Simulação\n\n\nCódigo\nset.seed(282829)\n\nrequire(MASS)\nrequire(systemfit)\nrequire(furrr)\n\n# parametros\nmu &lt;- c(2, 5, 10, 20)\nsigma &lt;- matrix(c(4, 2, 2, 2,\n                  2, 8, 3, 3,\n                  2, 3, 9, 3,\n                  2, 3, 3, 12), 4)\neq1 &lt;- y1 ~ x1 + x2  # equação 1\neq2 &lt;- y2 ~ x2 + x3  # equação 2\n\nplan('multisession', workers = 5)  # paralelização\n\nbetas &lt;- future_map(1:3000, ~{\n  # gerando amostra\n  pop &lt;- mvrnorm(30, mu, sigma)\n  erro &lt;- mvrnorm(30, c(0,0), matrix(c(40, 37, 37, 45), 2))\n  db &lt;- data.frame(\n    y1 = 1*pop[,1] + 2*pop[,2] + pop[,4] + erro[,1],\n    y2 = 3*pop[,2] + 4*pop[,3] + 2*pop[,4] + erro[,2],\n    x1 = pop[,1], x2 = pop[,2], x3 = pop[,3]\n  )\n\n  # ajustando modelos\n  fit_SUR &lt;- systemfit(list(mod1 = eq1, mod2 = eq2), method = 'SUR', data = db,\n                       control = systemfit.control(methodResidCov = 'noDfCor'))\n  fit_MQO &lt;- systemfit(list(mod1 = eq1, mod2 = eq2), data = db)\n\n  # resultados\n  c(coef(fit_MQO)[-c(1,4)], coef(fit_SUR)[-c(1,4)])\n}, .options=furrr_options(seed=TRUE)) |&gt;\n  do.call(rbind, args=_)\n\ncolnames(betas) &lt;- c('MQO1_x1', 'MQO1_x2', 'MQO2_x2', 'MQO2_x3',\n                     'SUR1_x1', 'SUR1_x2', 'SUR2_x2', 'SUR2_x3')\n\n\n\nTabela 1: Modelo 1 - Média (Desvio Padrão)\n\n\n\n\nTipo\n\\(\\beta_1\\)\n\\(\\beta_2\\)\n\n\n\n\nMQO\n\\(1.368\\; (0.756)\\)\n\\(2.299\\; (0.534)\\)\n\n\nSUR\n\\(0.982\\; (0.402)\\)\n\\(2.394\\; (0.502)\\)\n\n\n\n\n\nTabela 2: Modelo 2 - Média (Desvio Padrão)\n\n\n\n\nTipo\n\\(\\beta_2\\)\n\\(\\beta_3\\)\n\n\n\n\nMQO\n\\(3.592\\; (0.708)\\)\n\\(4.482\\; (0.643)\\)\n\n\nSUR\n\\(3.688\\; (0.664)\\)\n\\(4.230\\; (0.337)\\)"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#simulação-1",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#simulação-1",
    "title": "Modelo SUR",
    "section": "Simulação",
    "text": "Simulação"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#contexto",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#contexto",
    "title": "Modelo SUR",
    "section": "Contexto",
    "text": "Contexto\nO SUR espacial é uma técninca que incorpora simultaneamente efeitos espaciais e erros correlacionados entre as equações. Sua primeira menção ocorre em Anselin (\\(1998\\)): “Uma equação para cada período de tempo, a qual é estimada para uma seção transversão de unidades espaciais …”.\nO artigo utilizado para a análise disponibiliza o pacote spsur no R, o qual foi desenvolvido para considerar modelos de multiequações com efeitos espaciais e correlação residual."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#modelo",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#modelo",
    "title": "Modelo SUR",
    "section": "Modelo",
    "text": "Modelo\nO modelo geral SUR com dependência temporal SUR-GNM (general nesting model) é dado por:\n\\[\\tag{2}\\label{eq: surs}\n\\bs y_t = \\rho_t \\bs W_t^y \\bs y_t + \\bs X_t \\bs\\beta_t + \\bs W_t^x \\bs X_t^* \\bs\\theta_t + \\bs u_t\n\\]\ncom \\(\\bs u_t = \\lambda_t \\bs W_t^u \\bs  u_t + \\bs\\varepsilon_t\\), \\(\\E(\\bs\\varepsilon_t) = 0\\) e \\(\\E(\\bs\\varepsilon_t \\bs\\varepsilon_s') = \\sigma_{ts}I_N\\).\nonde:\n\n\\(\\bs X_t^*\\): é a matriz de regressoras sem o intercepto;\n\\(\\bs W_t^y\\), \\(\\bs W_t^x\\) e \\(\\bs W_t^u\\): são as matrizes de ponderação, com dimensão \\(n \\times n\\);"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-1",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-1",
    "title": "Modelo SUR",
    "section": "Modelo",
    "text": "Modelo\nComumente, utiliza-se \\(\\bs W = \\bs W_t^y = \\bs W_t^x = \\bs W_t^u\\). Então, o modelo \\(\\eqref{eq: surs}\\) pode ser reescrito como:\n\\[\\bs A \\bs y = \\bs X \\bs\\beta + (I_{g} \\otimes \\bs W)\\bs X^* \\bs\\theta + \\bs u\\] com \\(\\bs B \\bs u = \\bs\\varepsilon\\), \\(\\bs\\varepsilon \\sim \\norm(\\bs 0, \\bs\\Omega)\\).\n\\(\\bs y = \\begin{bmatrix} \\bs y_1 \\\\  \\vdots \\\\ \\bs y_g\\end{bmatrix}\\), \\(\\bs X = \\begin{bmatrix}\n    \\bs X_1 & \\bs0    & \\dots  & \\bs0\\\\\n    \\bs0    & \\bs X_2 & \\dots  & \\bs0\\\\\n    \\vdots  & \\vdots  & \\ddots & \\vdots\\\\\n    \\bs0    & \\bs0    & \\dots  & \\bs X_g\\\\\n\\end{bmatrix}\\), \\(\\bs\\beta = \\begin{bmatrix}\\bs\\beta_1 \\\\ \\vdots \\\\ \\bs\\beta_g\\end{bmatrix}\\), \\(\\bs\\theta = \\begin{bmatrix} \\bs\\theta_1 \\\\ \\vdots \\\\ \\bs\\theta_g \\end{bmatrix}\\), \\(\\bs u = \\begin{bmatrix} \\bs u_1 \\\\ \\vdots \\\\ \\bs u_g\\end{bmatrix}\\), \\(\\bs\\varepsilon = \\begin{bmatrix}\\bs\\varepsilon_1 \\\\ \\vdots \\\\ \\bs\\varepsilon_g \\end{bmatrix}\\)"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-2",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-2",
    "title": "Modelo SUR",
    "section": "Modelo",
    "text": "Modelo\nVeja que o modelo apresentado pode ter vários desdobramentos:\n\nSUR-SIM: modelo com independência espacial (SUR aprendido em aula), \\(\\rho_t = \\lambda_t =0\\), \\(\\bs\\theta_t = \\bs 0\\);\nSUR-SLX: modelo com defasagem espacial em \\(\\bs X\\), \\(\\rho_t = \\lambda_t = 0\\);\nSUR-SLM: modelo com defasagem espacial, \\(\\lambda_t = 0\\), \\(\\bs\\theta_t = \\bs0\\);\nSUR-SEM: modelo com erro espacial, \\(\\rho_t = 0\\), \\(\\bs\\theta_t = \\bs0\\);\nSUR-SDM: modelo espacial de Durbin, \\(\\lambda_t = 0\\);\nSUR-SDEM: modelo de erro espacial de Durbin; \\(\\rho_t = 0\\);\nSUR-SARAR: modelo de defasagem espacial com erros autorregressivos, \\(\\bs\\theta_t = \\bs0\\)."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#estimação-2",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#estimação-2",
    "title": "Modelo SUR",
    "section": "Estimação",
    "text": "Estimação\nÉ necessário impor que a diagonal de \\(\\bs W\\) seja preenchida por \\(0\\). Então, assumindo que os erros são normalmente distribuidos, basta maximizar a função log-verossimilhança, dada por:\n\\[\\begin{split}\nl(\\bs y; \\bs \\eta) = - \\frac{ng}{2} \\ln(2\\pi) - \\frac{n}{2}\\ln|\\bs\\Sigma| + g \\sum_{t=1}^{g} \\ln|\\bs B_t| + g \\sum_{t=1}^{g} \\ln|\\bs A_t| - \\\\-\\frac{1}{2}(\\bs A \\bs y - \\bs{\\bar X}[\\bs\\beta; \\bs\\theta]')'\\bs B' \\bs\\Omega^{-1} \\bs B(\\bs A \\bs y - \\bs{\\bar X}[\\bs\\beta; \\bs\\theta]')\n\\end{split}\\] onde \\(\\bs\\eta'=[\\bs\\beta'; \\bs\\theta'; \\rho_1;\\dots;\\rho_g; \\lambda_1;\\dots; \\lambda_g; \\sigma_{ij}]\\), \\(\\bs\\Lambda = diag(\\lambda_1, \\dots, \\lambda_g)\\), \\(\\bs\\Gamma = diag(\\rho_1,\\dots,\\rho_g)\\), \\(\\bs{\\bar X} = [\\bs X; (T_g \\otimes \\bs W) \\bs X^*]\\), \\(\\bs A = I_{n g} - \\bs\\Gamma \\otimes \\bs W\\), \\(\\bs B = I_{ng} - \\bs\\Lambda \\otimes \\bs W\\) e \\(\\bs\\Omega = \\bs\\Sigma \\otimes I_n\\).\n\nAo todo, temos que estimar \\(2 \\sum_{i=1}^g k_i + g + \\frac{g(g+1)}{2}\\) parâmetros 😨😱."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#conjunto-de-dados",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#conjunto-de-dados",
    "title": "Modelo SUR",
    "section": "Conjunto de Dados",
    "text": "Conjunto de Dados\nBanco de dados:\n\nspc: este banco de dados ilustra um exemplo classico da curva de Phillips retirado de Anselin (\\(1988\\), pp. \\(203\\)–\\(211\\)).\n\n\n\n\n\n\n\n\nCurva de Phillips\n\n\nA curva de Phillips é uma teoria macroeconômica que mostra a relação entre a taxa de desemprego e a taxa de inflação no curto prazo. Ela foi criada pelo economista neozelandês Alban William Phillips, a partir de dados do Reino Unido entre \\(1861\\) e \\(1957\\)."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#spc",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#spc",
    "title": "Modelo SUR",
    "section": "spc",
    "text": "spc\nObjetivo do estudo: analisar como diferentes variáveis impactaram as mudanças nas taxas salariais de \\(1981\\) para \\(1983\\) em \\(25\\) condados do sudoeste de Ohio.\nSejam as equações:\n\\[\\text{Equação 1:} \\quad WAGE_{83} = \\beta_{10} + \\beta_{11} UN_{83} + \\beta_{12} NMR_{83} + \\beta_{13} SMSA + u_{83}.\\]\n\\[\\text{Equação 2:} \\quad WAGE_{81} = \\beta_{20} + \\beta_{21} UN_{81} + \\beta_{22} NMR_{81} + \\beta_{23} SMSA + u_{81}.\\]\n\n\nWAGE: mudanças nas taxas salariais;\nUN: taxa de desemprego;\nNMR: taxa líquida de migração;\nSMSA: variável dummy (\\(1\\) para condados metropolitanos, \\(0\\) caso contrário)."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#spc-1",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#spc-1",
    "title": "Modelo SUR",
    "section": "spc",
    "text": "spc\n\n\nCódigo\n# Pacotes utilziados na aplicação\nrequire('gridExtra')\nrequire('ggplot2')\nrequire('dplyr')\nrequire('Rcpp')\nrequire('Gmisc')\nrequire('grid')\nrequire('spdep')\nrequire('sf')\nrequire('spsur')\n\n# Sintaxe\nspcformula &lt;- WAGE83 | WAGE81 ~ UN83 + NMR83 + SMSA | UN80 + NMR80 + SMSA\n\n# Criação do modelo SLM\nspcsur.slm &lt;- spsurml(formula = spcformula, data = spc, type = 'slm', method = 'eigen', listw = Wspc)\n\n\nneighbourhood matrix eigenvalues\nComputing eigenvalues ...\n\nInitial point:   log_lik:  113.198  rhos:  -0.472 -0.446 \nIteration:  1   log_lik:  114.088  rhos:  -0.506 -0.482 \nIteration:  2   log_lik:  114.098  rhos:  -0.506 -0.482 \nIteration:  3   log_lik:  114.099  rhos:  -0.505 -0.482 \nTime to fit the model:  0.67  seconds\nTime to compute covariances:  0.2  seconds \n\n\n\n\ntype: slm (Spatial Lag Model), que considera \\(\\lambda_t = 0\\) e \\(\\bs\\theta_t= 0\\), \\(\\forall t\\).\nmethod: “eigen”, utilizado para o cálculo do \\(|A|\\) e \\(|B|\\), sendo mais eficiente para amostras pequenas, mas computacionalmente caro para amostras grandes. Alternativa: realizar Fatoração LU ou Cholesky.\nlistw: lista de pesos espaciais, usada para definir vizinhanças em modelos espaciais."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#spc-2",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#spc-2",
    "title": "Modelo SUR",
    "section": "spc",
    "text": "spc\n\n\\(\\rho\\) é o parâmetro de autoregressão espacial, que indica a intensidade da dependência espacial nas variáveis dependentes. No nosso caso, \\(\\rho_1 = -0.505 &lt; 0\\) e \\(\\rho_2 = -0.482 &lt; 0\\) refletem uma dependência espacial negativa.\nBastou três iterações para a convergência do algoritmo, visto que não houveram mudanças significativas na função de verossimilhança e nos \\(\\rho\\)."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#spc-3",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#spc-3",
    "title": "Modelo SUR",
    "section": "spc",
    "text": "spc\n\n\nCódigo\nsummary(spcsur.slm)\n\n\nCall:\nspsurml(formula = spcformula, data = spc, listw = Wspc, type = \"slm\", \n    method = \"eigen\")\n\n \nSpatial SUR model type:  slm \n\nEquation  1 \n                Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)_1  1.4947178  0.2467450  6.0577 5.244e-07 ***\nUN83_1         0.8053338  0.2558760  3.1474  0.003249 ** \nNMR83_1       -0.5165301  0.2590369 -1.9940  0.053557 .  \nSMSA_1        -0.0072526  0.0118566 -0.6117  0.544484    \nrho_1         -0.5048488  0.2405967 -2.0983  0.042763 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nR-squared: 0.622 \n  Equation  2 \n                Estimate Std. Error t value  Pr(&gt;|t|)    \n(Intercept)_2  1.7088173  0.2925087  5.8419 1.028e-06 ***\nUN80_2        -0.6736472  0.3870209 -1.7406   0.09007 .  \nNMR80_2        0.7475735  0.3840119  1.9467   0.05919 .  \nSMSA_2         0.0013487  0.0241871  0.0558   0.95583    \nrho_2         -0.4816233  0.2557338 -1.8833   0.06754 .  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nR-squared: 0.4743 \n  \nVariance-Covariance Matrix of inter-equation residuals:                            \n  0.0003091646 -0.0003578072\n -0.0003578072  0.0015874436\nCorrelation Matrix of inter-equation residuals:                      \n  1.0000000 -0.5107461\n -0.5107461  1.0000000\n\n R-sq. pooled: 0.6601 \n Breusch-Pagan: 6.543  p-value: (0.0105) \n LMM: 0.50474  p-value: (0.477)\n\n\nO teste de Breusch-Pagan (da matriz de covariância diagonal) é o responsável por testar se a diagonal da matriz de variância contemporânea é igual à \\(0\\). Note que \\(\\text{p-valor} = 0.0105 &lt; 0.05\\), logo, as equações não possuem variâncias nulas, notável na saída da matriz de variância-covariância dos resíduos."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#spc-4",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#spc-4",
    "title": "Modelo SUR",
    "section": "spc",
    "text": "spc\nA saída LMM é o multiplicador Lagrangiano marginal, usado para testar se foram omitidos efeitos espaciais decorrentes dos modelos estimados. Nesse caso, como \\(\\text{p-valor} = 0.477 &gt; 0.05\\) não temos evidências suficientes para rejeitar a hipótese nula em favor da alternativa, isto é, não há evidência de autocorrelação espacial dos resíduos."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#intervalos-de-confiança-para-os-betas-da-primeira-equação",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#intervalos-de-confiança-para-os-betas-da-primeira-equação",
    "title": "Modelo SUR",
    "section": "Intervalos de confiança para os betas da primeira equação",
    "text": "Intervalos de confiança para os betas da primeira equação\n\n\nCódigo\nplot(spcsur.slm)"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#intervalos-de-confiança-para-os-betas-da-segunda-equação",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#intervalos-de-confiança-para-os-betas-da-segunda-equação",
    "title": "Modelo SUR",
    "section": "Intervalos de confiança para os betas da segunda equação",
    "text": "Intervalos de confiança para os betas da segunda equação"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#intervalos-de-confiança-para-rho",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#intervalos-de-confiança-para-rho",
    "title": "Modelo SUR",
    "section": "Intervalos de confiança para \\(\\rho\\)",
    "text": "Intervalos de confiança para \\(\\rho\\)"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#spc-5",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#spc-5",
    "title": "Modelo SUR",
    "section": "spc",
    "text": "spc\n\n\nCódigo\n# Razão de verossimilhanças\nspcsur.slm &lt;- spsurml(formula = spcformula, data = spc, type = 'slm',\n                      listw = Wspc, control = list(trace = FALSE))\nspcsur.sdm &lt;- spsurml(formula = spcformula, data = spc, type = 'sdm',\n                      listw = Wspc, control = list(trace = FALSE))\nanova(spcsur.slm, spcsur.sdm)\n\n\n             logLik df     AIC     BIC LRtest  p.val\nmodel 1: slm 114.10 13 -202.20 -219.19              \nmodel 2: sdm 116.78 19 -195.57 -220.40 5.3701 0.4973\n\n\nÉ possível comparar se modelos de diferentes tipos são equivalentes com a função anova. Foi considerado o modelo SUR-SDM, que além de incluir a defasagem espacial das variáveis respostas, inclui para as preditoras.\nComo \\(\\text{p-valor} = 0.4973 &gt; 0.05\\) não temos evidências suficientes para rejeitar a hipótese nula de que os modelos diferem.\nAlém disso, os critérios AIC e BIC penalizam modelos com mais parâmetros, busca-se valores menores desses critérios, indicando modelos mais parcimoniosos, isto é, uma explicação boa dos dados com o menor número possível de parâmetros."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#spc-6",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#spc-6",
    "title": "Modelo SUR",
    "section": "spc",
    "text": "spc\n\n\nCódigo\n# Teste de Wald, nesse caso estamos comparando se os interceptos são os mesmos\nR1 &lt;- matrix(c(1, 0, 0, 0, -1, 0, 0, 0), nrow = 1)\nb1 &lt;- matrix(0, ncol = 1)\nwald_betas(spcsur.slm, R = R1, b = b1)\n\n\n\n    Wald test on beta parameters\n\ndata:  spc\nWald test = 0.39767, df = 1, p-value = 0.5283\n\n\nOs interceptos não são estatisticamente diferentes entre si. Logo, uma especificação correta seria incluir o mesmo intercepto nas duas equações."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#spc-7",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#spc-7",
    "title": "Modelo SUR",
    "section": "spc",
    "text": "spc\n\n\nCódigo\n# adicionou um intercepto e fez um modelo restrito\nR1 &lt;- matrix(c(1, 0, 0, 0, -1, 0, 0, 0), nrow = 1)\nb1 &lt;- matrix(0, ncol = 1)\nspcsur.slm.restricted &lt;- spsurml(formula = spcformula, data = spc, type = 'slm', listw = Wspc,\n                                 R = R1, b = b1, control = list(trace = FALSE))\nprint(spcsur.slm.restricted)\n\n\n            coeff_1 pval_1 coeff_2 pval_2\n(Intercept)  1.5786  0.000  1.5786  0.000\nUN83         0.7946  0.003      NA     NA\nNMR83       -0.4873  0.064      NA     NA\nSMSA        -0.0056  0.632  0.0029  0.906\nUN80             NA     NA -0.6193  0.120\nNMR80            NA     NA  0.7083  0.079\nrho         -0.5856  0.006 -0.3705  0.052\n\n\nCódigo\n# Teste que compara ro1 de ro2 DO MODELO RESTRITO\nR2 &lt;- matrix(c(1, -1), nrow = 1)\nb2 &lt;- matrix(0, ncol = 1)\nwald_deltas(spcsur.slm.restricted, R = R2, b = b2)\n\n\n\n    Wald test on spatial delta parameters\n\ndata:  spc\nWald test = 16.932, df = 1, p-value = 3.874e-05\n\n\nConfirmação de que a especificação com dois parâmetros diferentes da dependencia espacial estava correta."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#verificando-normalidade-dos-resíduos",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#verificando-normalidade-dos-resíduos",
    "title": "Modelo SUR",
    "section": "Verificando normalidade dos resíduos",
    "text": "Verificando normalidade dos resíduos\n\n\nCódigo\nresiduos_eq1 &lt;- residuals(spcsur.slm)[[1]]\nqqnorm(residuos_eq1, main = 'QQ Plot dos Resíduos - Equação 1', col = 'blue', pch = 19, \n        ylab = 'Quantis Amostrais', xlab = 'Quantis Teóricos')\nqqline(residuos_eq1, col = 'red', lwd = 2)"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#verificando-normalidade-dos-resíduos-1",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#verificando-normalidade-dos-resíduos-1",
    "title": "Modelo SUR",
    "section": "Verificando normalidade dos resíduos",
    "text": "Verificando normalidade dos resíduos\n\n\nCódigo\nresiduos_eq2 &lt;- residuals(spcsur.slm)[[2]]\nqqnorm(residuos_eq2, main = 'QQ Plot dos Resíduos - Equação 2', col = 'blue', pch = 19, \n       ylab = 'Quantis Amostrais', xlab = 'Quantis Teóricos')\nqqline(residuos_eq2, col = 'red', lwd = 2)"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#ncovr",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#ncovr",
    "title": "Modelo SUR",
    "section": "NCOVR",
    "text": "NCOVR\nO segundo conjunto de dados, retirado de Ballter et al. (\\(2001\\)), refere-se as taxas de homicídios em \\(3085\\) condados dos EUA para quatro anos (\\(1960\\), \\(1970\\), \\(1980\\) e \\(1990\\)).\n\\[\\begin{align*}\n\\text{Equação 1:}& \\quad HR_{80} = \\beta_{10} + \\beta_{11} PS_{83} + \\beta_{12} UE_{83} + \\beta_{13} SMSA + u_{HR}\\\\\n\\text{Equação 2:}& \\quad DV_{80} = \\beta_{20} + \\beta_{21} PS_{80} + \\beta_{22} UE_{80} + \\beta_{23} SOUTH + u_{EDV}\\\\\n\\text{Equação 3:}& \\quad FP_{79} = \\beta_{3   0} + \\beta_{31} PS_{80} + u_{FR}\n\\end{align*}\\]\n\n\n\\(HR_{80}:\\) taxa de homicídios por \\(100.000\\) habitantes em \\(1980\\).\n\\(PS_{80}:\\) estrutura populacional de \\(1980\\).\n\\(UE_{80}:\\) taxa de desemprego em \\(1980\\).\n\\(DV_{80}:\\) taxa de divórcio em \\(1980\\).\n\\(FP_{79}:\\) porcentagem de famílias abaixo da linha de pobreza em \\(1980\\).\n\\(SOUTH:\\) variável dummy (\\(1\\) para condados do sul, \\(0\\) caso contrário)."
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#distribuição-espacial-das-taxas-de-homicídios-em-1980",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#distribuição-espacial-das-taxas-de-homicídios-em-1980",
    "title": "Modelo SUR",
    "section": "Distribuição espacial das taxas de homicídios em \\(1980\\)",
    "text": "Distribuição espacial das taxas de homicídios em \\(1980\\)\n\n\nCódigo\ndata('NCOVR', package = 'spsur')\n\nco &lt;- sf::st_coordinates(sf::st_centroid(NCOVR.sf))\nncovrlw &lt;- nb2listw(knn2nb(knearneigh(co, k = 10, longlat = TRUE)), style = 'W')\nq &lt;- quantile(NCOVR.sf$HR80)\nNCOVR.sf$Quantile&lt;- as.factor((NCOVR.sf$HR80 &gt; q[2]) + (NCOVR.sf$HR80 &gt; q[3]) +\n  (NCOVR.sf$HR80 &gt;=  q[4]) + 1)\nggplot(data = NCOVR.sf) +\n  geom_sf(aes(fill = Quantile), color = 'black', size = .2) +\n  theme_bw(base_size = 6) +\n  scale_fill_manual(values = c('#FFFEDE', '#FFDFA2', '#FFA93F', '#D5610D'))+\n  theme(plot.margin = unit(c(0.1, 0.1, 0.1, 0.1), 'cm'))"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#simulação-2",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#simulação-2",
    "title": "Modelo SUR",
    "section": "Simulação",
    "text": "Simulação\nForam gerados 537 indivíduos para a simulação dentro de um hexágono regular. Além disso, foram consideradas as equações:\n\\[\\text{Equação 1:} \\quad Y_1 = 1 + 0.2 \\bs WY_1 + 2X_{11} + 3 X_{12} + \\varepsilon_1\\] \\[\\text{Equação 2:} \\quad Y_2 = 1 + 0.8 \\bs W Y_2 - X_{21} + 0.5 X_{22} + \\varepsilon_2\\] \\[\\operatorname{cor}(\\varepsilon_1, \\varepsilon_2) = 0.5\\]"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-errado---sim",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-errado---sim",
    "title": "Modelo SUR",
    "section": "Modelo Errado - SIM",
    "text": "Modelo Errado - SIM\n\n\nCódigo\nset.seed(123)\nrequire(patchwork)  # arrumar gráficos ggplot\n\nsfc &lt;- st_sfc(st_polygon(list(rbind(c(0, 0), 1:0, c(1, 1), 0:1, c(0, 0)))))\nhexs &lt;- st_make_grid(sfc, cellsize = 0.049, square=F)\nhexs.sf &lt;- st_sf(hexs)\n\n# gerando dados\nW &lt;- nb2listw(poly2nb(as(hexs.sf, 'Spatial'), queen = FALSE))  # Matriz de ponderação (rook criteria)\nSigma &lt;- matrix(c(1  , 0.5,\n                  0.5, 1), 2)\nBetas &lt;- c(1,2,3,2,-1,0.5)\nrho &lt;- c(0.2,0.8)\ndb &lt;- dgp_spsur(Sigma = Sigma, Betas = Betas, Thetas = NULL, lambda = NULL, rho = rho, Tm = 1,\n                G = 2, N = 537, p = 3, listw = W)  # SUR-SLM\n\n# estimando modelos\n## SUR-SIM - modelo errado\nsur_sim &lt;- spsurml(Y = db$Y, X = db$X, type = 'sim', G = 2, Tm = 1, N = 537, p = 3,\n                   listw = W, control = list(trace = FALSE))\nres &lt;- residuals(sur_sim)\nhexs.sf$res.eq1 &lt;- res[[1]]\nhexs.sf$res.eq2 &lt;- res[[2]]\n\n### gráfico do resíduo\ntema &lt;- theme_bw(base_size = 12) + theme(legend.position = 'none', axis.text.x = element_blank(),\n        axis.text.y = element_blank(), axis.ticks = element_blank(), plot.title = element_text(hjust=0.5))\n\ngraf1 &lt;- ggplot(data = hexs.sf) + geom_sf(aes(fill = res.eq1), color = 'gray', size = .1) + tema +\n            labs(title='Equação 1')\ngraf2 &lt;- ggplot(data = hexs.sf) + geom_sf(aes(fill = res.eq2), color = 'gray', size = .1) + tema +\n            labs(title='Equação 2')\ngraf1 + graf2"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-certo---slm",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-certo---slm",
    "title": "Modelo SUR",
    "section": "Modelo Certo - SLM",
    "text": "Modelo Certo - SLM\n\n\nCódigo\n## SUR-SLM - modelo certo\nsur_slm &lt;- spsurml(Y = db$Y, X = db$X, type = 'slm', G = 2, Tm = 1, N = 537, p = 3,\n                   listw = W, control = list(trace = FALSE))\nres &lt;- residuals(sur_slm)\nhexs.sf$res.eq1 &lt;- res[[1]]\nhexs.sf$res.eq2 &lt;- res[[2]]\n\n### gráfico do resíduo\ngraf1 &lt;- ggplot(data = hexs.sf) + geom_sf(aes(fill = res.eq1), color = 'gray', size = .1) + tema +\n            labs(title='Equação 1')\ngraf2 &lt;- ggplot(data = hexs.sf) + geom_sf(aes(fill = res.eq2), color = 'gray', size = .1) + tema +\n            labs(title='Equação 2')\ngraf1 + graf2"
  },
  {
    "objectID": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-certo",
    "href": "scripts/apresentacoes/ME715_Apresentacao.html#modelo-certo",
    "title": "Modelo SUR",
    "section": "Modelo Certo",
    "text": "Modelo Certo\n\n\nCódigo\nsummary(sur_slm)\n\n\nCall:\nspsurml(listw = W, type = \"slm\", X = db$X, Y = db$Y, G = 2, N = 537, \n    Tm = 1, p = 3, control = list(trace = FALSE))\n\n \nSpatial SUR model type:  slm \n\nEquation  1 \n           Estimate Std. Error t value  Pr(&gt;|t|)    \nIntercep_1 1.050658   0.052887  19.866 &lt; 2.2e-16 ***\nX1_1       2.009105   0.038119  52.706 &lt; 2.2e-16 ***\nX1_2       2.984680   0.035863  83.225 &lt; 2.2e-16 ***\nrho_1      0.161010   0.021984   7.324 4.746e-13 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nR-squared: 0.9292 \n  Equation  2 \n            Estimate Std. Error  t value  Pr(&gt;|t|)    \nIntercep_2  2.066812   0.219397   9.4204 &lt; 2.2e-16 ***\nX2_1       -0.978890   0.038798 -25.2304 &lt; 2.2e-16 ***\nX2_2        0.469211   0.036570  12.8305 &lt; 2.2e-16 ***\nrho_2       0.791277   0.021862  36.1946 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nR-squared: 0.8029 \n  \nVariance-Covariance Matrix of inter-equation residuals:                    \n 0.9994753 0.5283877\n 0.5283877 1.0096595\nCorrelation Matrix of inter-equation residuals:                    \n 1.0000000 0.5259921\n 0.5259921 1.0000000\n\n R-sq. pooled: 0.9632 \n Breusch-Pagan: 148.6  p-value: (3.56e-34) \n LMM: 6.4689  p-value: (0.011)"
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regressão-não-paramétrica-1",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regressão-não-paramétrica-1",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Regressão Não Paramétrica",
    "text": "Regressão Não Paramétrica\nConsidere o modelo: \\[\ny_i = f(x_i) + e_i \\;,\\spc i = 1,\\dots,n\n\\] com \\(e_i \\overset{iid}{\\sim} \\norm(0, \\sigma^2)\\).\n\nEm Problemas de regressão paramérica, assume-se uma forma funcional para \\(f\\), por exemplo, \\(f(x) = \\beta_0 + \\beta_1 x + \\dots + \\beta_p x^p\\).\n\nJá em problemas de regressão não paramétrica, \\(f\\) é desconhecida. Logo, para estimar \\(f\\) utiliza-se bases como ondaletas, splines entre outras.\n\nObservações \\(y\\) são resultantes da função verdadeira \\(f\\) contaminada de um erro \\(e\\)."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regressão-não-paramétrica-2",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regressão-não-paramétrica-2",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Regressão Não Paramétrica",
    "text": "Regressão Não Paramétrica\nAdemais, em termos vetoriais, o modelo é dado por: \\[\n\\bs y = \\bs f + \\bs e\n\\] onde\n\\[\n\\bs y = \\begin{bmatrix}y_1\\\\\\vdots\\\\y_n\\end{bmatrix}, \\hspace{2.5cm} \\bs f = \\begin{bmatrix}f(x_1)\\\\\\vdots\\\\f(x_n)\\end{bmatrix}, \\hspace{2.5cm} \\bs e =\\begin{bmatrix}e_1\\\\\\vdots\\\\e_n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regressão-não-paramétrica-3",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regressão-não-paramétrica-3",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Regressão Não Paramétrica",
    "text": "Regressão Não Paramétrica\n\nVantagens:\n\nMaior flexibilidade;\nCaptura padrões complexos e não lineares;\nAdapta-se aos dados, sendo especialmente útil quando há pouco conhecimento prévio sobre o modelo.\n\n\nDesvantagens:\n\nMenor interpretabilidade;\nNecessita de uma maior quantidade de dados."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Ondaleta",
    "text": "Ondaleta\nSeja \\(\\psi \\in \\mathbb{L}_2(\\RR) = \\{f: \\RR \\to \\RR \\mid \\int f^2 &lt; \\infty\\}\\) uma função que satisfaça a condição \\[\nC_\\psi = \\int_\\RR \\frac{\\lvert\\Psi(\\omega)\\rvert^2}{\\lvert\\omega\\rvert} d\\omega &lt; \\infty\n\\] onde \\(\\Psi\\) é a transformada de Fourier de \\(\\psi\\). Então, dizemos que \\(\\psi\\) é uma ondaleta (mãe)."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta-1",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta-1",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Ondaleta",
    "text": "Ondaleta\nAlém disso, uma vez que a ondaleta foi definida, pode-se gerar ondaletas através de operações de dilatação e translação: \\[\n\\psi_{jk}(x) = 2^{j/2} \\psi (2^jx - k) \\;, \\spc j,k \\in \\ZZ\n\\]\nComo \\(\\{\\psi_{jk}\\}_{j,k \\in \\ZZ}\\) formam uma base ortonormal para \\(\\mathbb{L}_2(\\RR)\\), qualquer função \\(f \\in \\mathbb{L}_2(\\RR)\\) pode ser representada por: \\[\nf(x) = \\sum_{j,k \\in \\ZZ} d_{jk} \\psi_{jk}(x)\n\\] onde \\(d_{jk}\\) é chamado de coeficiente de ondaleta (“detalhes”).\n\n\nA partir da ondaleta mãe podemos gerar demais ondaletas.\nO conjunto de ondaletas formam uma base."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta-2",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta-2",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Ondaleta",
    "text": "Ondaleta\nAlém disso, também existe a ondaleta pai ou função escala \\(\\phi\\), a qual fazemos translações e dilatações: \\[\n\\phi_{jk}(x) = 2^{j/2} \\phi (2^jx - k) \\;, \\spc j,k \\in \\NN\n\\]\nCom isso, para determinano nível \\(j_0 \\in \\ZZ\\) (resolução primária), uma função \\(f\\) pode ser escrita por: \\[\nf(x) = \\sum_{k \\in \\ZZ} c_{j_0k} \\phi_{j_0k}(x) + \\sum_{j \\geq j_0} \\sum_{k \\in \\ZZ} d_{jk}\\psi_{jk}(x)\n\\] onde \\(c_{jk}\\) é chamado de coeficiente da função escala.\n\nResslatar nome dos coeficientes:\n\n\\(c_{jk}\\): coeficiente da função escala;\n\\(d_{jk}\\): coeficiente da ondaleta;"
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta-3",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta-3",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Ondaleta",
    "text": "Ondaleta\nDo ponto de vista matricial, considerando um vetor de dados \\(\\bs y \\in \\RR^n\\) diádico, isto é, \\(n = 2^J\\), \\(J \\in \\NN\\). É possível aplicar a transformada discreta de ondaletas (DWT), representada pela matriz \\(W\\), de forma que \\[\n\\bs d = W \\bs y\n\\] onde \\(\\bs d\\) representa os coeficientes empíricos de ondaleta.\nAlém disso, como \\(W\\) é uma matriz ortogonal (\\(W' = W^{-1}\\)), a transformada discreta de ondaletas inversa (IDWT) é dada por: \\[\n\\bs y = W'\\bs d\n\\]\nAdemais, pela ortogonalidade de \\(W\\), temos que \\(\\|\\bs d\\|^2_2 = \\|\\bs y\\|^2_2\\) (Relação de Parseval).\n\n\n\\(W\\) é uma matriz ortogonal, então a transformada inversa é dada pela sua transposta;\n\\(W\\) é uma matriz ortogonal \\(\\Rightarrow\\) relação de Parseval;\nQuadrado da normal 2 é chamado de energia."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta-4",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#ondaleta-4",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Ondaleta",
    "text": "Ondaleta\n\n\n\n\n\nFigura 1: Funções ondaleta de Daubechies \\(\\psi\\) e escala \\(\\phi\\) para 1 (Haar), 2, 5 e 8 momentos nulos.\n\n\n\n\n\n\n\n\nApenas as ondaletas de Haar tem forma fechada;\nQuanto mais momentos nulos, mais ela oscila."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-1",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-1",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Exemplo 1: DWT",
    "text": "Exemplo 1: DWT\nOndaleta (mãe) de Haar: \\[\n\\psi(x) =\n\\begin{cases}\n  1  &,  \\; x \\in \\left[0, \\frac{1}{2}\\right)\\\\\n  -1 &,  \\; x \\in \\left[\\frac{1}{2}, 1\\right)\\\\\n  0  &,  \\; c{.}c.\n\\end{cases}\n\\]\n\nFunção escala de Haar:\n\\[\n\\phi(x) =\n\\begin{cases}\n  1  &,  \\; x \\in [0, 1]\\\\\n  0  &,  \\; c{.}c.\n\\end{cases}\n\\]"
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-2",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-2",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Exemplo 1: DWT",
    "text": "Exemplo 1: DWT\nAgora que fixamos \\(\\psi\\) e \\(\\phi\\), falta apenas definir como calcular \\(c_{jk}\\) e \\(d_{jk}\\). Neste caso (considerando a ondaleta de Haar), eles podem ser calculados na primeira aplicação por:\n\\[\nd_{jk} = \\frac{1}{\\sqrt{2}}(y_{2k} - y_{2k-1}) \\hspace{2cm} \\text{ e } \\hspace{2cm} c_{jk} = \\frac{1}{\\sqrt{2}}(y_{2k} + y_{2k-1})\n\\] e, nas demais, por: \\[\nd_{jk} = \\frac{1}{\\sqrt{2}}(c_{j+1,2k} - c_{j+1, 2k-1}) \\hspace{2cm} \\text{ e } \\hspace{2cm}\nc_{jk} = \\frac{1}{\\sqrt{2}}(c_{j+1,2k} + c_{j+1,2k-1})\n\\]\n\n\nDiferenças e somas normalizadas."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-3",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-3",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Exemplo 1: DWT",
    "text": "Exemplo 1: DWT\nConsiderando o vetor de dados \\(\\bs y = (1, 1, 7, 9, 2, 8, 8, 6)'\\), a Figura 1 apresenta graficamente a DWT.\n\nFigura 2: Exemplo visual da DWT (Nason, 2008)."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-4",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-4",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Exemplo 1: DWT",
    "text": "Exemplo 1: DWT\n\n\n\n\n\nFigura 3: Decomposição dos coeficientes de ondaletas. por nível de resolução."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-5",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#exemplo-1-dwt-5",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Exemplo 1: DWT",
    "text": "Exemplo 1: DWT\nA matriz \\(W\\) que faz essa transformação para \\(n=8\\) observações é dada por:\n\\[\nW =\n\\begin{bmatrix}\n\\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4}\\\\\n\\frac{-1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\frac{-1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 & 0 & 0 & 0\\\\\n0 & 0 &0 & 0 & \\frac{-1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} & 0 & 0\\\\\n0 & 0 & 0 & 0 &0 & 0 & \\frac{-1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}}\\\\\n\\frac{-1}{2} & \\frac{-1}{2} & \\frac{1}{2} & \\frac{1}{2} & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 &\\frac{-1}{2} & \\frac{-1}{2} & \\frac{1}{2} & \\frac{1}{2}\\\\\n\\frac{-\\sqrt2}{4} & \\frac{-\\sqrt2}{4} & \\frac{-\\sqrt2}{4} & \\frac{-\\sqrt2}{4} & \\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4} & \\frac{\\sqrt2}{4}\n\\end{bmatrix}\n\\]"
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#modelo-1",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#modelo-1",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Modelo",
    "text": "Modelo\nPortanto, retomando ao modelo de regressão não paramétrica, temos que: \\[\n\\bs y = \\bs f + \\bs e\n\\] com \\(e_i \\overset{iid}{\\sim} \\norm(0, \\sigma^2)\\).\nEntão, multiplicando por \\(W\\) vamos ter o nosso modelo no domínio das ondaletas, dado por: \\[\n\\bs d = \\bs\\theta + \\bs\\varepsilon\n\\] onde\n\n\\(\\bs d = W \\bs y\\) é o vetor dos coeficientes empíricos de ondaleta;\n\\(\\bs \\theta = W \\bs f\\) é o vetor dos coeficientes verdadeiros;\n\\(\\bs\\varepsilon = W \\bs e\\) é o erro, o qual tem suas propriedades preservadas, i.e., \\(\\varepsilon_i \\overset{iid}{\\sim} \\norm(0,\\sigma^2)\\)."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#modelo-2",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#modelo-2",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Modelo",
    "text": "Modelo\nLogo, o problema de estimar a função \\(f\\) foi substituido por estimar os coeficientes de ondaletas \\(\\bs\\theta\\) através dos coeficientes empíricos \\(\\bs d\\).\n\nPara isso, a abordagem mais tradicional é a utilização das regras de shrinkage, neste caso, a limiarização (thresholding), a qual consiste em anular coeficientes suficientemente pequenos. As duas propostas mais comuns são a do limiar duro (hard threshold) e limiar suave (soft threshold).\n\nA intuição desta técnica consiste em que o vetor de coeficientes de ondaleta normalmente são esparsos. Além disso, coeficientes grandes estão associados às características importantes da função, como mínimos e máximos locais, descontinuidades entre outras. Enquanto que a parte mais suave da função se associa aos coeficientes nulos.\n\n\nSubistituimos estimar \\(f\\) por estimar \\(d\\), para isso, usamos regras de encolhimento.\nGrandes coeficientes de ondaletas são associados à características importantes da função."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#modelo-3",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#modelo-3",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Modelo",
    "text": "Modelo\n\nRegra do limiar duro: \\[\n\\delta^H(d) =\n\\begin{cases}\n  0 &, \\text{ se } \\lvert d \\rvert \\leq \\lambda\\\\\n  d &, \\text{ se } \\lvert d \\rvert &gt; \\lambda\n\\end{cases}\n\\]\n\nFigura 4: Regra do limiar duro.\n\n\n\n\n\nRegra do limiar suave: \\[\n\\delta^S(d) =\n\\begin{cases}\n  0 &, \\text{ se } \\lvert d \\rvert \\leq \\lambda\\\\\n  \\operatorname{sgn}(d) (\\lvert d \\rvert - \\lambda) &, \\text{ se } \\lvert d \\rvert &gt; \\lambda\n\\end{cases}\n\\]\n\nFigura 5: Regra do limiar suave."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#modelo-4",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#modelo-4",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Modelo",
    "text": "Modelo\nExistem diversas estratégias para determinar \\(\\lambda\\), algumas delas são:\n\nValidação cruzada.\nUniversal Threshold: utilizando o MAD (median absolute deviation) para estimar \\(\\sigma\\), temos: \\[\n\\lambda = \\sigma \\sqrt{2 \\ln(n)}\n\\]\nSURE Threshold: escolhe o \\(\\lambda\\) que minimiza o estimador não viesado do risco de Stein \\(SURE(\\lambda, \\bs y)\\), dado por: \\[\nSURE(\\lambda; \\mathbf{y}) = n - 2\\#\\{i:|y_i| \\leq \\lambda\\} + \\sum_{i}(\\min\\{|y_i|, \\lambda\\})^2\n\\]"
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#recapitulando",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#recapitulando",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Recapitulando",
    "text": "Recapitulando\n  \n\nFigura 6: Resumo do procedimento de estimação."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#funções-de-teste",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#funções-de-teste",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Funções de Teste",
    "text": "Funções de Teste\n\n\n\n\n\nFigura 7: Funções de teste de Donoho e Johnstone."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#razão-sinal-ruído-snr",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#razão-sinal-ruído-snr",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Razão Sinal-Ruído (SNR)",
    "text": "Razão Sinal-Ruído (SNR)\nA razão sinal-ruído (SNR) é dada por:\n\\[\nSNR = \\frac{\\operatorname{SD}(\\text{sinal})}{\\operatorname{SD}(\\text{ruído})} = \\frac{\\operatorname{SD}(f)}{\\operatorname{SD}(\\varepsilon)}\n\\] Para estudos simulacionais, é interessante fixar a razão sinal-ruído para determinar o desepenho da téninca para aquela \\(SNR\\) e permitir a comparação com outros métodos.\n\n\n\n\n\nFigura 8: Comparação entre diferentes SNR. Azul representa a função verdadeira e preto a amostra gerada."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#gerando-amostra",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#gerando-amostra",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Gerando Amostra",
    "text": "Gerando Amostra\n\n\n\n\n\nFigura 9: Amostra gerada com \\(SNR = 3\\). Azul representa a função verdadeira e preto a amostra."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#coeficientes-de-ondaletas",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#coeficientes-de-ondaletas",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Coeficientes de Ondaletas",
    "text": "Coeficientes de Ondaletas\n\n\n\n\n\nFigura 10: Coeficientes de ondaletas empíricos (a.) e estimados (b.) por nível de resolução.\n\n\n\n\n\n\n\nA quantidade de coeficientes nulos foi de 890."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#estimativa-da-bumps",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#estimativa-da-bumps",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Estimativa da Bumps",
    "text": "Estimativa da Bumps\n\n\n\n\n\nFigura 11: Estimativa da bumps utilizando ondaleta de Daubechies com 2 momentos nulos e política de validação cruzada. Azul representa a função verdadeira e preto a função estimada.\n\n\n\n\n\n\n\nRessaltar que a recuperação foi relativamente boa considerando \\(SNR=3\\), o qual já é considerado um caso com muito ruído."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#priore-spike-and-slab-1",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#priore-spike-and-slab-1",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Priore Spike and Slab",
    "text": "Priore Spike and Slab\nExistem também propostas bayesianas para o problema, por exemplo, a apresentada em Barrios (2025), cuja priori atribuída é: \\[\\pi(\\theta) = \\alpha \\delta_0(\\theta) + (1 - \\alpha) g(\\theta; \\beta) \\hspace{1.2cm} \\text{ e } \\hspace{1.2cm} \\sigma^2 \\sim Exp(\\lambda)\\] onde \\(\\alpha \\in (0,1)\\), \\(\\lambda &gt; 0\\), \\(\\delta_0\\) é o delta de Dirac com massa em \\(0\\) e \\(g(d; \\beta)\\) é a função densidade da Epanechnikov Generalizada, dada por: \\[\ng(x; \\beta) = \\frac{3}{4\\beta^3}(\\beta^2 - x^2) \\ind_{(-\\beta, \\beta)}^{(x)}\n\\] com \\(\\beta &gt; 0\\).    \nVale ressaltar que, diferente das estratégias apresentadas anteriormente, propostas bayesianas não zeram os coeficientes, apenas encolhem.\n\nPropostas Bayesianas apenas encolhem, mas não zeram os coeficientes."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regra-de-encolhimento",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regra-de-encolhimento",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Regra de Encolhimento",
    "text": "Regra de Encolhimento\n\n\n\n\n\nFigura 12: Densidade da Epanechnikov Generalizada para \\(\\beta = 2\\), \\(3\\) e \\(5\\)."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regra-de-encolhimento-1",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regra-de-encolhimento-1",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Regra de Encolhimento",
    "text": "Regra de Encolhimento\nCom isso, é possível estabelecer a distribuição a posteriori \\(\\theta \\mid d\\) de cada coeficiente:\n\\[\\begin{align*}\n\\pi(\\theta \\mid d) &\\propto \\pi(\\theta) \\LL(\\theta; d)\\\\\n&\\propto \\left[\\alpha \\delta_0(\\theta) + (1 - \\alpha) g(\\theta; \\tau) \\right]\\exp\\left\\{\\frac{-1}{2\\sigma^2} (d - \\theta)^2\\right\\}\n\\end{align*}\\]\nA regra de encolhimento é dada pela esperança da posteriori, calculando obtém-se que: \\[\n\\small\n\\delta(d) =\n\\dfrac{\n  (1-\\alpha) \\frac{3 \\sqrt{2\\lambda}}{8 \\beta^3}\n  \\left[ \\frac{2 \\lambda\\beta^2 + 3\\beta\\sqrt{2\\lambda} + 3}{2\\lambda^2}\n  \\left( e^{-\\sqrt{2\\lambda}(\\beta - d)} - e^{-\\sqrt{2\\lambda}(\\beta+d)} \\right)\n  + \\frac{(\\lambda\\beta^2 - 3) d\\sqrt{2\\lambda} - d^3\\lambda\\sqrt{2\\lambda}}{\\lambda^2}\n  \\right]\n}{\n  \\alpha \\frac{\\sqrt{2l}}{2} e^{-\\sqrt{(2l)}\\lvert d \\rvert}\n  \\left(0, \\frac{1}{\\sqrt{2\\lambda}} \\right) +\n  (1 - \\alpha) \\frac{3\\sqrt{2\\lambda}}{8\\beta^3} \\left[\\frac{\\beta}{\\lambda}\n  \\left( e^{-\\sqrt{2\\lambda}(\\beta + d)} + e^{-\\sqrt{2\\lambda}(\\beta-d)} \\right)\n+ \\frac{2}{\\sqrt{2\\lambda}} \\left(\\beta^2 - d^2 - \\frac{1}{\\lambda}\\right) \\right]\n}\n\\]    Por sorte, esta regra tem forma explicita, no geral, elas não são tratáveis analiticamente, sendo necessário utilizar métodos Monte Carlo para aproximá-la.\n\nÉ raro a regra ser tratável analiticamente, normalmente, é necessário utilizar métodos Monte Carlo.\nCuriosidade, esta regra foi desenvolvida para altos níveis de razão sinal-ruído."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regra-de-encolhimento-2",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#regra-de-encolhimento-2",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Regra de Encolhimento",
    "text": "Regra de Encolhimento\n\n\n\n\n\nFigura 13: Regra de encolhimento variando os parámetro \\(\\alpha\\), \\(\\beta\\) e \\(\\lambda\\), separadamente."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#contextualização",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#contextualização",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Contextualização",
    "text": "Contextualização\nA epilepsia é um distúrbio neurológico caracterizado pela ocorrência de convulsões, causadas por uma atividade neuronal excessiva ou sincronizada no cérebro.\n\nEssas crises podem resultar em problemas psiquiátricos, quedas, déficits cognitivos e aumento do risco de morte.\n\nUma das estratégias mais promissoras é a detecção do estado pré-ictal, período que antecede uma crise.\n\nIdentificá-lo pode permitir ações preventivas e reduzir os impactos negativos das convulsões."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#aplicação-1",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#aplicação-1",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Aplicação",
    "text": "Aplicação\nEste estudo utiliza dados do eletroencefalograma (EEG) do couro cabeludo de \\(14\\) pacientes, registrados com eletrodos reutilizáveis de prata e ouro.\n\nOs dados foram retirados de Detti et al. (2020), da Unidade de Neurologia e Neurofisiologia da Universidade de Siena, Itália.\n\nO banco apresenta variáveis como gênero, idade, número de canais de EEG, quantidade de crises e o tempo das gravações.\n\nPara este estudo, foi seleiconado o paciente \\(5\\) com tamanho amostral correspondente a \\(n = 2^{15} = 32.768\\) observações.\n\n\nTeve que ser feito um corte nos dados para o vetor ficar diádico."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#aplicação-2",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#aplicação-2",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Aplicação",
    "text": "Aplicação\n\n\n\n\n\nFigura 15: Função observada (preto) e função recuperada (azul).\n\n\n\n\n\n\n\n\nPossível aplicação em detecção de pontos de mudança para determinar possível ataque epilético;\nFazer link com Análise de Dados Funcionais."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#simulação-1",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#simulação-1",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Simulação",
    "text": "Simulação\nNa simulação foram comparados os cenários:\n\n\\(10.000\\) replicações;\n\\(n = 32\\) e \\(128\\) pontos;\n\\(SNR = 3\\) e \\(SNR = 7\\);\nFunções de teste: Bumps e SpaHet;\nMétodos aplicados:\n\nLimiarização com política SURE para escolha de limiar;\nProposta bayesiana com \\(\\alpha = 0.8\\), \\(\\lambda_{n=32} \\approx 0.34\\) e \\(\\lambda_{n=128} \\approx 1.3\\);\nSuaviazação por splines (método “concorrente”).\n\nForam utilizadas as ondaletas de Daubechies com \\(5\\) momentos nulos.\n\n\n\\(\\beta\\) é utilizado como o maior coeficiente de ondaleta em módulo, para que a regra faça sentido, devido ao domínio da Epanechnikov."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#funções-usadas-na-simulação",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#funções-usadas-na-simulação",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Funções Usadas na Simulação",
    "text": "Funções Usadas na Simulação\n\n\n\n\n\nFigura 16: Funções de teste utilizadas na simulação.\n\n\n\n\n\n\n\nRessaltar:\n\nBumps é “mal comportada” (muitos picos);\nSpaHet é “bem comportada” (suave)."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#splines",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#splines",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Splines",
    "text": "Splines\nNeste método, busca-se estimar uma função \\(g\\), tal que: \\[\n\\sum_{i=1}^n (y_i - g(x_i))^2 + \\lambda\\int g''(t) dt\n\\] com \\(\\lambda \\leq 0\\).\nA função \\(g\\) que minimiza a condição é denominada de spline de suavização. Além disso, o parâmetro \\(\\lambda\\) controla a suavidade da função, onde:\n\n\\(\\lambda = 0\\): gera uma \\(g\\) interpoladora (não há penalização);\n\\(\\lambda \\to \\infty\\): aumenta a suavização de \\(g\\) até chegar em uma reta.\n\n\n\nToda observação é um knot, trocando o problema de posicioná-los (spline) por determinar \\(\\lambda\\) (suavização por spline).\nGera uma função suave.\nLembra métodos como LASSSO e Ridge."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#resultados",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#resultados",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Resultados",
    "text": "Resultados\n\nTabela 1: Resultado da simulação com AMSE (SD).\n\n\n\n\n\n\nAMSE: Average Mean Squared Error."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#resultados-1",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#resultados-1",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Resultados",
    "text": "Resultados\n\n\n\n\n\nFigura 17: Box Plot do MSE."
  },
  {
    "objectID": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#vantagens-e-desvantagens-das-ondaletas",
    "href": "scripts/apresentacoes/regressaoII/ME861_Apresentacao.html#vantagens-e-desvantagens-das-ondaletas",
    "title": "Regressão Não Paramétrica Através de Ondaletas",
    "section": "Vantagens e Desvantagens das Ondaletas",
    "text": "Vantagens e Desvantagens das Ondaletas\n\nVantages:\n\nModela funções com discontinuidades, picos entre outras características;\nRepresentação esparsa;\nGrande flexibilidade, já que existem diversos tipos de ondaletas, para diversos problemas.\n\n\nDesvantagens:\n\nNo geral, não apresentam forma fechada.\nMaior dificuldade em interpretação, comparado à metodos tradicionais.\n\n\n\nNo geral, outros métodos tem dificuldade em modelar características de discontinuidade e afins."
  }
]