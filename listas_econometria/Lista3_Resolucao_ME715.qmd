---
title: 'Lista 3 - Econometria - ME715'
lang: pt
crossref:
  eq-prefix: 'Eq. '
format:
  html:
    toc: false
    embed-resources: true
    theme:
      dark: darkly
      light: flatly
---

::: {.hidden}
```{css, echo=FALSE}
p {
  text-align: justify
}

/* Ocultar o texto padrão de referência */
.reference-cross {
    display: none;
}

/* Estilo para mostrar apenas o número da equação */
.equation-number {
    color: #007BFF; /* Cor personalizada */
    font-weight: bold; /* Negrito */
}

details {
    border: 2px solid #272726; /* Borda ao redor do elemento */
    border-radius: 5px; /* Arredondamento das bordas */
    padding: 10px; /* Espaçamento interno */
    margin-top: 10px; /* Espaçamento superior */
}
```

$$
\DeclareMathOperator{\tr}{tr}
\DeclareMathOperator{\posto}{posto}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Var}{\mathbb{V}}
\DeclareMathOperator{\bbE}{\mathbb{E}}
\newcommand{\bs}[1]{\boldsymbol{#1}}
\newcommand{\RR}{\mathbb{R}}
$$
:::

::: {.callout-note icon=false}
# Questão 1

Prove que $\boldsymbol M = I - X(X'X)^{-1}X'$ é simétrica e idempotente.
:::

Vamos provar que $\bs{M}$ é simétrica, isto é, $\bs{M}' = \bs{M}$:


$$
\bs{M}' = (I - X(X'X)^{-1}X')' = I - X(X'X)^{-1}X' = \bs{M}
$$

Vamos provar que $\bs{M}$ é idempotente, isto é, $\bs{M}\bs{M} = \bs{M}$:

\begin{align*}
  \bs{M}\bs{M} &= (I - X(X'X)^{-1}X') (I - X(X'X)^{-1}X')\\
  &= I - X(X'X)^{-1}X' -  X(X'X)^{-1}X' + X(X'X)^{-1}X' X(X'X)^{-1}X'\\
  &= I - 2 X(X'X)^{-1}X' + X(X'X)^{-1}X'\\
  &= I - X(X'X)^{-1}X'\\
  &= \bs{M}
\end{align*}

Portanto, fica provado que $\bs{M}$ é simétrica e idempotente.

<br><br>


::: {.callout-note icon=false}
# Questão 2

Mostre que $s^2 = \displaystyle\frac{\hat u' \hat u}{n-k-1}$ é um estimador não viesado para $\sigma^2$.
:::

::: {.callout-tip collapse=false}
# Resultados auxiliares

::: {#thm-esp_forma_quadratica}

# Esperança de forma quadrática

Seja $X$ um vetor aleatório $n\times 1$ com média $\mu$ e covariância $\Sigma$ e $A \in \mathbb{R}^{n \times n}$ uma matriz simétrica. Então, a esperança de uma forma quadrática $X'AX$ é dada por:
$$\bbE(X'AX) = \tr(A\Sigma) + \mu'A\mu$$
:::

<details>
<summary> Proof </summary>
Note que $X'AX$ é uma matriz $1 \times 1$, então
$$
\mathbb E(X'AX) = \bbE(\tr(X'AX)) = \bbE(\tr(AXX')) = \tr(A\bbE(XX'))
$$

Além disso, como $\Var(X) = \bbE(XX') - \bbE(X)\bbE(X)' \Rightarrow \bbE(XX') = \Var(X) + \bbE(X)\bbE(X)'$, então

\begin{align}
\bbE(X'AX) &= \tr(A\bbE(XX'))\\
           &= \tr(A[\Var(X) + \bbE(X)\bbE(X)'])\\
           &= \tr(A(\Sigma + \mu\mu'))\\
           &= \tr(A\Sigma) + \tr(A\mu\mu')\\
           &= \tr(A\Sigma) + \tr(\mu'A\mu')\\
           &= \tr(A\Sigma) + \mu'A\mu
\end{align}
</details>


::: {#thm-saber_lee}
## Saber and Lee - Teorema 3.1, p. 40

Suponha que $X$ é uma matriz $n \times p$ de posto $p$, tal que $H = X(X'X)^{-1}X'$. Então,

  1. $H$ e $I - H$ são simétricas e idempotentes;
  1. $\posto(I -H) = \tr(I - H) = n - p$;
  1. $HX = X$.

:::
:::


Queremos mostrar que $\mathbb E(s^2) = \displaystyle\mathbb E\left(\frac{\hat{u}'\hat{u}}{n-k-1}\right) = \sigma^2$.

Seja $H = X(X'X)^{-1}X'$ e $\bs{M} = I - H$, então, $\hat{u} = Y - \hat Y = (I - H)Y = MY$. Além disso, sabemos que $\bs{M}$ é simétrica e idempotente, como provado na Questão 1. Assim,

$$\bbE(\hat{u}'\hat{u}) = \bbE(Y'M'MY) = \bbE(Y'MMY) =\bbE(Y'MY)$$

Então, pelo @thm-esp_forma_quadratica e @thm-saber_lee, temos
\begin{align*}
\bbE(Y'MY) &= \tr(M\sigma^2I) + (X\beta)'MX\beta\\
           &= \sigma^2 \tr(M) + \beta'X'(I - H)X\beta\\
           &= \sigma^2 \tr(I - H) + \beta'X'X\beta -
                \beta'X'HX\beta\\
           &= \sigma^2 (n - (k + 1)) + \beta'X'X\beta -
                \beta'X'X\beta\\
           &= \sigma^2(n - k - 1)
\end{align*}


Portanto,
$$
\bbE(s^2) = \displaystyle\bbE \left( \frac{\hat{u}'\hat{u}}{n-k-1} \right) = \frac{1}{n-k-1}\bbE (Y'MY) = \frac{\sigma^2(n-k-1)}{n-k-1} = \sigma^2
$$

<br><br>


::: {.callout-note icon=false}
# Questão 3

Mostre que $R^2$ nunca diminui quando incluimos novas variáveis no modelo.
:::

Considere o modelo

$$ 
Y = X\beta + u
$$ {#eq-Q3_mod1}
Além disso, considere adicionar uma ou mais novas variáveis ao modelo @eq-Q3_mod1, então, temos que:

$$
Y = X_0 \beta_0 + X \beta_1 + v
$$ {#eq-Q3_mod2}

Calculando a $SQE$ do modelo da @eq-Q3_mod2:

\begin{align*}
\hat v' \hat v = (Y - \hat Y)'(Y - \hat Y)\\
      &= (X\hat\beta + \hat u - X_0 \hat\beta_0 - X\hat\beta_1)'(X\hat\beta + \hat u - X_0 \hat\beta_0 - X\hat\beta_1)\\
      &= (X (\hat\beta - \hat\beta_1) - X_0\hat\beta_0 + \hat u)'(X (\hat\beta - \hat\beta_1) - X_0\hat\beta_0 + \hat u)\\
      &= \underbrace{(X (\hat\beta - \hat\beta_1) - X_0\hat\beta_0)'(X (\hat\beta - \hat\beta_1) - X_0\hat\beta_0)}_{A} + 2 \hat u'(X (\hat\beta - \hat\beta_1) - X_0\hat\beta_0) + \hat u' \hat u\\
      & = A + 2 \hat u'X (\beta - \hat\beta_1) - 2\hat u' X_0\hat\beta_0 + \hat u' \hat u
\end{align*}

Como provado na Lista 2, desde que $\beta$ seja estimado por OLS, então, $\hat u'X = \hat u'X_0 = 0'$. Logo,

Logo,
$$\hat v' \hat v = A + \hat u' \hat u$$

Veja que $A = (X (\hat\beta - \hat\beta_1) - X_0 \hat\beta_0)^2_2 \Rightarrow A \geq 0$. Portanto,

$$
\hat v' \hat v \leq \hat u' \hat u
$$

Como $R^2 = 1 - \frac{SQE}{SQT}$, temos

$$1 - \frac{\hat v' \hat v}{SQT} \geq 1 - \frac{\hat u' \hat u}{SQT}$$

Portanto, adicionar preditores, não diminuirá o $R^2$.

<br><br>


::: {.callout-note icon=false}
# Questão 4

Utilize o *dataset* `htv`, estime o modelo de regressão
$$educ = \beta_0 + \beta_1 motheduc + \beta_2fatheduc + \beta_3abil + \beta_4 abil^2 + u$$
e interprete os resultados.
:::

```{r, include=F}
# configurações para utilziar Python e Julia
require(JuliaCall)
require(reticulate)
```

::: {.panel-tabset}

## R
```{r, warning=F, message=F}
# package e banco de dados
require(wooldridge)
data(htv)

fit_r <- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), data=htv)
coef(fit_r)
```


## Python
```{python}
# packages
import statsmodels.formula.api as smf
import wooldridge as woo

# lendo dados
htv = woo.dataWoo('htv')
htv = r['htv']  # leitura alternativa (direto do r)

# ajustando modelo
fit_py = smf.ols(formula='educ ~ motheduc + fatheduc + abil + I(abil**2)',
                 data=htv).fit().params
fit_py
```


## Julia
```{julia}
# packages
using WooldridgeDatasets, DataFrames, Statistics, GLM

# dados
htv = DataFrame(wooldridge("htv"));

fit_julia = lm(@formula(educ ~ motheduc + fatheduc + abil + abil^2), htv);
coeficientes = DataFrame(Beta = coefnames(fit_julia), Valor = coef(fit_julia))
```

:::

<br><br>


::: {.callout-note icon=false}
# Questão 5

Pove o Teorema FWL.


::: {#thm-fwl}
## Frisch-Waugh-Lovell

Sejam os modelos
$$
Y = X_1 \beta_1 + X_2\beta_2 + {u} \qquad \text{e} \qquad M_1Y = M_1 X_2 \beta_2 + \nu
$$
em que $M_1 = I - X_1 (X_1' X_1)^{-1}X_1'$. Então,

1. $\hat\beta_2$ em ambas regressões são numericamente idênticos;
1. $\hat u$ e $\hat \nu$ são numericamente idênticos.
:::
:::

Considere o modelo

$$
Y = X\beta + u
$$ {#eq-modelo}
onde $X$ é uma matriz $n \times p$, $\beta$ é um vetor de dimensão $k \times 1$ e $u$ é o vetor do erro aleatório de dimensão $n \times 1$. Note que $X$ e $\beta$ podem ser escritos como:

$$
X = \begin{bmatrix}X_1 & X_2\end{bmatrix} \hspace{1cm} \text{e} \hspace{1cm} \beta = \begin{bmatrix} \beta_1\\ \beta_2 \end{bmatrix}
$$

onde $X_1$ é uma matriz $n \times k_1$, $X_2$ é uma matriz $n \times (p - k_1)$, $\beta_1$ é um vetor de dimensão $k_1 \times 1$ e $\beta_2$ é um vetor de dimensão $(k - k_1) \times 1$. Então, o modelo da @eq-modelo pode ser escrito como

$$
Y = X_1\beta_1 + X_2\beta_2 + u 
$$

Vamos provar que $\hat \nu$ e $\hat u$ são numericamente identicos. Para isso considere

$$
Y = X_1 \hat\beta_1 + X_2 \hat\beta_2 + \hat u 
$$ {#eq-obs}

Além disso, sabemos (da Lista 2) que $X' \hat u = \bar 0$, então
$$
\begin{bmatrix} X_1' \\ X_2' \end{bmatrix} \hat u = \begin{bmatrix}\bar0 \\ \bar0 \end{bmatrix} \Rightarrow X_1' \hat u = X_2' \hat u = \bar0 \tag{5} \label{eq-lista2}
$$

Multiplicando a @eq-obs por $M_1$, temos

\begin{align*}
M_1 Y &= M_1X_1\hat\beta_1 + M_1X_2\hat\beta_2 + M_1\hat u\\
      &= (I - X_1(X_1' X_1)^{-1}X_1')X_1\hat\beta_1 + M_1X_2\hat\beta_2 + (I - X_1(X_1' X_1)^{-1}X_1')\hat u\\
      &= (X_1 - X_1 (X_1' X_1)^{-1}X_1'X_1) \beta_1 + M_1X_2\hat\beta_2 + \hat u - X_1(X_1' X_1)^{-1} \underbrace{X_1'\hat u}_{\begin{array}{c}\bar0\\\text{(por \eqref{eq-lista2})}\end{array}}\\
      &= M_1X_2\hat\beta_2 + \hat u
\end{align*}

Logo,

$$M_1 Y = M_1X_2\hat\beta_2 + \hat \nu$$
com $\hat\nu = \hat u$. Portanto, fica provado que $\hat\nu$ e $\hat u$ são numericamente identicos.

Além disso, note que $\hat\beta_2$ não foi alterado, e foi possível chegar em um modelo a partir, logo, $\hat\beta_2$ é numericamente idêntico em ambos modelos.

<br><br>


# Referências

Saber A.F.G.; Lee, A.J. (2003). *Linear Regression Analysis*. Segunda edição. Wiley.

