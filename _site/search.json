[
  {
    "objectID": "listas_econometria/Lista 3 - Resolucao - ME715.html",
    "href": "listas_econometria/Lista 3 - Resolucao - ME715.html",
    "title": "Lista 3 - Econometria - ME715",
    "section": "",
    "text": "\\[\n\\DeclareMathOperator{\\tr}{tr}\n\\DeclareMathOperator{\\posto}{posto}\n\\DeclareMathOperator{\\Cov}{Cov}\n\\DeclareMathOperator{\\Var}{\\mathbb{V}}\n\\DeclareMathOperator{\\bbE}{\\mathbb{E}}\n\\newcommand{\\bs}[1]{\\boldsymbol{#1}}\n\\newcommand{\\RR}{\\mathbb{R}}\n\\]\n\n\n\n\n\n\n\nQuestão 1\n\n\n\nProve que \\(\\boldsymbol M = I - X(X'X)^{-1}X'\\) é simétrica e idempotente.\n\n\nVamos provar que \\(\\bs{M}\\) é simétrica, isto é, \\(\\bs{M}' = \\bs{M}\\):\n\\[\n\\bs{M}' = (I - X(X'X)^{-1}X')' = I - X(X'X)^{-1}X' = \\bs{M}\n\\]\nVamos provar que \\(\\bs{M}\\) é idempotente, isto é, \\(\\bs{M}\\bs{M} = \\bs{M}\\):\n\\[\\begin{align*}\n  \\bs{M}\\bs{M} &= (I - X(X'X)^{-1}X') (I - X(X'X)^{-1}X')\\\\\n  &= I - X(X'X)^{-1}X' -  X(X'X)^{-1}X' + X(X'X)^{-1}X' X(X'X)^{-1}X'\\\\\n  &= I - 2 X(X'X)^{-1}X' + X(X'X)^{-1}X'\\\\\n  &= I - X(X'X)^{-1}X'\\\\\n  &= \\bs{M}\n\\end{align*}\\]\nPortanto, fica provado que \\(\\bs{M}\\) é simétrica e idempotente.\n\n\n\n\n\n\n\nQuestão 2\n\n\n\nMostre que \\(s^2 = \\displaystyle\\frac{\\hat u' \\hat u}{n-k-1}\\) é um estimador não viesado para \\(\\sigma^2\\).\n\n\n\n\n\n\n\n\nResultados auxiliares\n\n\n\n\nTeorema 1 (esperança de forma quadrática) Seja \\(X\\) um vetor aleatório \\(n\\times 1\\) com média \\(\\mu\\) e covariância \\(\\Sigma\\) e \\(A \\in \\mathbb{R}^{n \\times n}\\) uma matriz simétrica. Então, a esperança de uma forma quadrática \\(X'AX\\) é dada por: \\[\\bbE(X'AX) = \\tr(A\\Sigma) + \\mu'A\\mu\\]\n\n\n\nProof\n\nNote que \\(X'AX\\) é uma matriz \\(1 \\times 1\\), então \\[\n\\mathbb E(X'AX) = \\bbE(\\tr(X'AX)) = \\bbE(\\tr(AXX')) = \\tr(A\\bbE(XX'))\n\\]\nAlém disso, como \\(\\Var(X) = \\bbE(XX') - \\bbE(X)\\bbE(X)' \\Rightarrow \\bbE(XX') = \\Var(X) + \\bbE(X)\\bbE(X)'\\), então\n\\[\\begin{align}\n\\bbE(X'AX) &= \\tr(A\\bbE(XX'))\\\\\n           &= \\tr(A[\\Var(X) + \\bbE(X)\\bbE(X)'])\\\\\n           &= \\tr(A(\\Sigma + \\mu\\mu'))\\\\\n           &= \\tr(A\\Sigma) + \\tr(A\\mu\\mu')\\\\\n           &= \\tr(A\\Sigma) + \\tr(\\mu'A\\mu')\\\\\n           &= \\tr(A\\Sigma) + \\mu'A\\mu\n\\end{align}\\]\n\n\nTeorema 2 (Saber and Lee - Teorema 3.1, p. 40) Suponha que \\(X\\) é uma matriz \\(n \\times p\\) de posto \\(p\\), tal que \\(H = X(X'X)^{-1}X'\\). Então,\n\n\\(H\\) e \\(I - H\\) são simétricas e idempotentes;\n\\(\\posto(I -H) = \\tr(I - H) = n - p\\);\n\\(HX = X\\).\n\n\n\n\nQueremos mostrar que \\(\\mathbb E(s^2) = \\displaystyle\\mathbb E\\left(\\frac{\\hat{u}'\\hat{u}}{n-k-1}\\right) = \\sigma^2\\).\nSeja \\(H = X(X'X)^{-1}X'\\) e \\(\\bs{M} = I - H\\), então, \\(\\hat{u} = Y - \\hat Y = (I - H)Y = MY\\). Além disso, sabemos que \\(\\bs{M}\\) é simétrica e idempotente, como provado na Questão 1. Assim,\n\\[\\bbE(\\hat{u}'\\hat{u}) = \\bbE(Y'M'MY) = \\bbE(Y'MMY) =\\bbE(Y'MY)\\]\nEntão, pelo Teorema 1 e Teorema 2, temos \\[\\begin{align*}\n\\bbE(Y'MY) &= \\tr(M\\sigma^2I) + (X\\beta)'MX\\beta\\\\\n           &= \\sigma^2 \\tr(M) + \\beta'X'(I - H)X\\beta\\\\\n           &= \\sigma^2 \\tr(I - H) + \\beta'X'X\\beta -\n                \\beta'X'HX\\beta\\\\\n           &= \\sigma^2 (n - (k + 1)) + \\beta'X'X\\beta -\n                \\beta'X'X\\beta\\\\\n           &= \\sigma^2(n - k - 1)\n\\end{align*}\\]\nPortanto, \\[\n\\bbE(s^2) = \\displaystyle\\bbE \\left( \\frac{\\hat{u}'\\hat{u}}{n-k-1} \\right) = \\frac{1}{n-k-1}\\bbE (Y'MY) = \\frac{\\sigma^2(n-k-1)}{n-k-1} = \\sigma^2\n\\]\n\n\n\n\n\n\n\nQuestão 3\n\n\n\nMostre que \\(R^2\\) nunca diminui quando incluimos novas variáveis no modelo.\n\n\nConsidere o modelo\n\\[\nY = X\\beta + u\n\\tag{1}\\] Além disso, considere adicionar uma ou mais novas variáveis ao modelo Eq. 1, então, temos que:\n\\[\nY = X_0 \\beta_0 + X \\beta_1 + v\n\\tag{2}\\]\nCalculando a \\(SQE\\) do modelo da Eq. 2:\n\\[\\begin{align*}\n\\hat v' \\hat v = (Y - \\hat Y)'(Y - \\hat Y)\\\\\n      &= (X\\hat\\beta + \\hat u - X_0 \\hat\\beta_0 - X\\hat\\beta_1)'(X\\hat\\beta + \\hat u - X_0 \\hat\\beta_0 - X\\hat\\beta_1)\\\\\n      &= (X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0 + \\hat u)'(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0 + \\hat u)\\\\\n      &= \\underbrace{(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0)'(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0)}_{A} + 2 \\hat u'(X (\\hat\\beta - \\hat\\beta_1) - X_0\\hat\\beta_0) + \\hat u' \\hat u\\\\\n      & = A + 2 \\hat u'X (\\beta - \\hat\\beta_1) - 2\\hat u' X_0\\hat\\beta_0 + \\hat u' \\hat u\n\\end{align*}\\]\nComo provado na Lista 2, desde que \\(\\beta\\) seja estimado por OLS, então, \\(\\hat u'X = \\hat u'X_0 = 0'\\). Logo,\nLogo, \\[\\hat v' \\hat v = A + \\hat u' \\hat u\\]\nVeja que \\(A = (X (\\hat\\beta - \\hat\\beta_1) - X_0 \\hat\\beta_0)^2_2 \\Rightarrow A \\geq 0\\). Portanto,\n\\[\n\\hat v' \\hat v \\leq \\hat u' \\hat u\n\\]\nComo \\(R^2 = 1 - \\frac{SQE}{SQT}\\), temos\n\\[1 - \\frac{\\hat v' \\hat v}{SQT} \\geq 1 - \\frac{\\hat u' \\hat u}{SQT}\\]\nPortanto, adicionar preditores, não diminuirá o \\(R^2\\).\n\n\n\n\n\n\n\nQuestão 4\n\n\n\nUtilize o dataset htv, estime o modelo de regressão \\[educ = \\beta_0 + \\beta_1 motheduc + \\beta_2fatheduc + \\beta_3abil + \\beta_4 abil^2 + u\\] e interprete os resultados.\n\n\n\nRPythonJulia\n\n\n\n# package e banco de dados\nrequire(wooldridge)\ndata(htv)\n\nfit_r &lt;- lm(educ ~ motheduc + fatheduc + abil + I(abil^2), data=htv)\ncoef(fit_r)\n\n(Intercept)    motheduc    fatheduc        abil   I(abil^2) \n 8.24022643  0.19012613  0.10893866  0.40146240  0.05059901 \n\n\n\n\n\n# packages\nimport statsmodels.formula.api as smf\nimport wooldridge as woo\n\n# lendo dados\nhtv = woo.dataWoo('htv')\nhtv = r['htv']  # leitura alternativa (direto do r)\n\n# ajustando modelo\nfit_py = smf.ols(formula='educ ~ motheduc + fatheduc + abil + I(abil**2)',\n                 data=htv).fit().params\nfit_py\n\nIntercept       8.240226\nmotheduc        0.190126\nfatheduc        0.108939\nabil            0.401462\nI(abil ** 2)    0.050599\ndtype: float64\n\n\n\n\n\n# packages\nusing WooldridgeDatasets, DataFrames, Statistics, GLM\n\n# dados\nhtv = DataFrame(wooldridge(\"htv\"));\n\nfit_julia = lm(@formula(educ ~ motheduc + fatheduc + abil + abil^2), htv);\ncoeficientes = DataFrame(Beta = coefnames(fit_julia), Valor = coef(fit_julia))\n\n5×2 DataFrame\n Row │ Beta         Valor\n     │ String       Float64\n─────┼───────────────────────\n   1 │ (Intercept)  8.24023\n   2 │ motheduc     0.190126\n   3 │ fatheduc     0.108939\n   4 │ abil         0.401462\n   5 │ abil ^ 2     0.050599\n\n\n\n\n\n\n\n\n\n\n\n\nQuestão 5\n\n\n\nPove o Teorema FWL.\n\nTeorema 3 (Frisch-Waugh-Lovell) Sejam os modelos \\[\nY = X_1 \\beta_1 + X_2\\beta_2 + {u} \\qquad \\text{e} \\qquad M_1Y = M_1 X_2 \\beta_2 + \\nu\n\\] em que \\(M_1 = I - X_1 (X_1' X_1)^{-1}X_1'\\). Então,\n\n\\(\\hat\\beta_2\\) em ambas regressões são numericamente idênticos;\n\\(\\hat u\\) e \\(\\hat \\nu\\) são numericamente idênticos.\n\n\n\n\nConsidere o modelo\n\\[\nY = X\\beta + u\n\\tag{3}\\] onde \\(X\\) é uma matriz \\(n \\times p\\), \\(\\beta\\) é um vetor de dimensão \\(k \\times 1\\) e \\(u\\) é o vetor do erro aleatório de dimensão \\(n \\times 1\\). Note que \\(X\\) e \\(\\beta\\) podem ser escritos como:\n\\[\nX = \\begin{bmatrix}X_1 & X_2\\end{bmatrix} \\hspace{1cm} \\text{e} \\hspace{1cm} \\beta = \\begin{bmatrix} \\beta_1\\\\ \\beta_2 \\end{bmatrix}\n\\]\nonde \\(X_1\\) é uma matriz \\(n \\times k_1\\), \\(X_2\\) é uma matriz \\(n \\times (p - k_1)\\), \\(\\beta_1\\) é um vetor de dimensão \\(k_1 \\times 1\\) e \\(\\beta_2\\) é um vetor de dimensão \\((k - k_1) \\times 1\\). Então, o modelo da Eq. 3 pode ser escrito como\n\\[\nY = X_1\\beta_1 + X_2\\beta_2 + u\n\\]\nVamos provar que \\(\\hat \\nu\\) e \\(\\hat u\\) são numericamente identicos. Para isso considere\n\\[\nY = X_1 \\hat\\beta_1 + X_2 \\hat\\beta_2 + \\hat u\n\\tag{4}\\]\nAlém disso, sabemos (da Lista 2) que \\(X' \\hat u = \\bar 0\\), então \\[\n\\begin{bmatrix} X_1' \\\\ X_2' \\end{bmatrix} \\hat u = \\begin{bmatrix}\\bar0 \\\\ \\bar0 \\end{bmatrix} \\Rightarrow X_1' \\hat u = X_2' \\hat u = \\bar0 \\tag{5} \\label{eq-lista2}\n\\]\nMultiplicando a Eq. 4 por \\(M_1\\), temos\n\\[\\begin{align*}\nM_1 Y &= M_1X_1\\hat\\beta_1 + M_1X_2\\hat\\beta_2 + M_1\\hat u\\\\\n      &= (I - X_1(X_1' X_1)^{-1}X_1')X_1\\hat\\beta_1 + M_1X_2\\hat\\beta_2 + (I - X_1(X_1' X_1)^{-1}X_1')\\hat u\\\\\n      &= (X_1 - X_1 (X_1' X_1)^{-1}X_1'X_1) \\beta_1 + M_1X_2\\hat\\beta_2 + \\hat u - X_1(X_1' X_1)^{-1} \\underbrace{X_1'\\hat u}_{\\begin{array}{c}\\bar0\\\\\\text{(por \\eqref{eq-lista2})}\\end{array}}\\\\\n      &= M_1X_2\\hat\\beta_2 + \\hat u\n\\end{align*}\\]\nLogo,\n\\[M_1 Y = M_1X_2\\hat\\beta_2 + \\hat \\nu\\] com \\(\\hat\\nu = \\hat u\\). Portanto, fica provado que \\(\\hat\\nu\\) e \\(\\hat u\\) são numericamente identicos.\nAlém disso, note que \\(\\hat\\beta_2\\) não foi alterado, e foi possível chegar em um modelo a partir, logo, \\(\\hat\\beta_2\\) é numericamente idêntico em ambos modelos.\n\n\nReferências\nSaber A.F.G.; Lee, A.J. (2003). Linear Regression Analysis. Segunda edição. Wiley."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "jsicas.github.io",
    "section": "",
    "text": "Link da Lista 3\nabout\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\n\n1 + 1\n\n[1] 2"
  }
]